# ViT实现部分说明
论文地址[论文](paper/vit.pdf)  
ViT从论文到实现都比较简单  
值得注意的几个实现细节：
1. 激活函数的使用。  
激活函数在transformer结构中大多使用**GELU函数**，图像处理和attention部分不存在激活函数，仅有feedforward部分存在，且使用顺序为**Linear->GELU->Dropout**。  
即一般而言，激活之后再dropout。  
2. nn.Sequential是能够顺序执行的各个层；nn.ModuleList是多个层对象组成的list，需要对其进行循环才能够逐步执行；
3. attention计算的细节。  
attention的key query value可以通过一个[seq_len, d_model*3]的对象进行计算，可能会增加效率；  
使用einops能够简单地分为多头，即'b s (h d) -> b h s d'；恢复则是'b h s d -> b s (h d)'
4. 图像切块的细节。  
vit中最为核心的就是将图像进行切块，本实现采用rearrange方法，对于一个b, h, w, c的图片，采用'b (h p_h) (w p_w) c -> b (h w) (p_h p_w c)'就完成了图像分割
5. pe和cls token
pe和cls token的初始化都是用nn.Parameter(torch.randn(size=())) 这样能够构建出能进行参数更新的对象  
cls token需要使用repeat方法进行batch次重复，以和feature拼接，使用repeat的原因是单个参数更新相同，满足对每个样本cls完全相同的要求。

