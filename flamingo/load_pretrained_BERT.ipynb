{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from bert4keras.backend import keras\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.tokenizers import Tokenizer\n",
    "from bert4keras.snippets import to_array\n",
    "import tensorflow as tf\n",
    "import modeling\n",
    "import utils\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.python import pywrap_tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "config_path = 'chinese_L-12_H-768_A-12/bert_config.json'\n",
    "checkpoint_path = 'chinese_L-12_H-768_A-12/bert_model.ckpt'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/wangyang/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 15:09:43.888141: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-07-04 15:09:43.908993: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f867e8d75a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-07-04 15:09:43.909007: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "model = build_transformer_model(config_path, checkpoint_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['Input-Token',\n 'Input-Segment',\n 'Embedding-Token',\n 'Embedding-Segment',\n 'Embedding-Token-Segment',\n 'Embedding-Position',\n 'Embedding-Norm',\n 'Embedding-Dropout',\n 'Transformer-0-MultiHeadSelfAttention',\n 'Transformer-0-MultiHeadSelfAttention-Dropout',\n 'Transformer-0-MultiHeadSelfAttention-Add',\n 'Transformer-0-MultiHeadSelfAttention-Norm',\n 'Transformer-0-FeedForward',\n 'Transformer-0-FeedForward-Dropout',\n 'Transformer-0-FeedForward-Add',\n 'Transformer-0-FeedForward-Norm',\n 'Transformer-1-MultiHeadSelfAttention',\n 'Transformer-1-MultiHeadSelfAttention-Dropout',\n 'Transformer-1-MultiHeadSelfAttention-Add',\n 'Transformer-1-MultiHeadSelfAttention-Norm',\n 'Transformer-1-FeedForward',\n 'Transformer-1-FeedForward-Dropout',\n 'Transformer-1-FeedForward-Add',\n 'Transformer-1-FeedForward-Norm',\n 'Transformer-2-MultiHeadSelfAttention',\n 'Transformer-2-MultiHeadSelfAttention-Dropout',\n 'Transformer-2-MultiHeadSelfAttention-Add',\n 'Transformer-2-MultiHeadSelfAttention-Norm',\n 'Transformer-2-FeedForward',\n 'Transformer-2-FeedForward-Dropout',\n 'Transformer-2-FeedForward-Add',\n 'Transformer-2-FeedForward-Norm',\n 'Transformer-3-MultiHeadSelfAttention',\n 'Transformer-3-MultiHeadSelfAttention-Dropout',\n 'Transformer-3-MultiHeadSelfAttention-Add',\n 'Transformer-3-MultiHeadSelfAttention-Norm',\n 'Transformer-3-FeedForward',\n 'Transformer-3-FeedForward-Dropout',\n 'Transformer-3-FeedForward-Add',\n 'Transformer-3-FeedForward-Norm',\n 'Transformer-4-MultiHeadSelfAttention',\n 'Transformer-4-MultiHeadSelfAttention-Dropout',\n 'Transformer-4-MultiHeadSelfAttention-Add',\n 'Transformer-4-MultiHeadSelfAttention-Norm',\n 'Transformer-4-FeedForward',\n 'Transformer-4-FeedForward-Dropout',\n 'Transformer-4-FeedForward-Add',\n 'Transformer-4-FeedForward-Norm',\n 'Transformer-5-MultiHeadSelfAttention',\n 'Transformer-5-MultiHeadSelfAttention-Dropout',\n 'Transformer-5-MultiHeadSelfAttention-Add',\n 'Transformer-5-MultiHeadSelfAttention-Norm',\n 'Transformer-5-FeedForward',\n 'Transformer-5-FeedForward-Dropout',\n 'Transformer-5-FeedForward-Add',\n 'Transformer-5-FeedForward-Norm',\n 'Transformer-6-MultiHeadSelfAttention',\n 'Transformer-6-MultiHeadSelfAttention-Dropout',\n 'Transformer-6-MultiHeadSelfAttention-Add',\n 'Transformer-6-MultiHeadSelfAttention-Norm',\n 'Transformer-6-FeedForward',\n 'Transformer-6-FeedForward-Dropout',\n 'Transformer-6-FeedForward-Add',\n 'Transformer-6-FeedForward-Norm',\n 'Transformer-7-MultiHeadSelfAttention',\n 'Transformer-7-MultiHeadSelfAttention-Dropout',\n 'Transformer-7-MultiHeadSelfAttention-Add',\n 'Transformer-7-MultiHeadSelfAttention-Norm',\n 'Transformer-7-FeedForward',\n 'Transformer-7-FeedForward-Dropout',\n 'Transformer-7-FeedForward-Add',\n 'Transformer-7-FeedForward-Norm',\n 'Transformer-8-MultiHeadSelfAttention',\n 'Transformer-8-MultiHeadSelfAttention-Dropout',\n 'Transformer-8-MultiHeadSelfAttention-Add',\n 'Transformer-8-MultiHeadSelfAttention-Norm',\n 'Transformer-8-FeedForward',\n 'Transformer-8-FeedForward-Dropout',\n 'Transformer-8-FeedForward-Add',\n 'Transformer-8-FeedForward-Norm',\n 'Transformer-9-MultiHeadSelfAttention',\n 'Transformer-9-MultiHeadSelfAttention-Dropout',\n 'Transformer-9-MultiHeadSelfAttention-Add',\n 'Transformer-9-MultiHeadSelfAttention-Norm',\n 'Transformer-9-FeedForward',\n 'Transformer-9-FeedForward-Dropout',\n 'Transformer-9-FeedForward-Add',\n 'Transformer-9-FeedForward-Norm',\n 'Transformer-10-MultiHeadSelfAttention',\n 'Transformer-10-MultiHeadSelfAttention-Dropout',\n 'Transformer-10-MultiHeadSelfAttention-Add',\n 'Transformer-10-MultiHeadSelfAttention-Norm',\n 'Transformer-10-FeedForward',\n 'Transformer-10-FeedForward-Dropout',\n 'Transformer-10-FeedForward-Add',\n 'Transformer-10-FeedForward-Norm',\n 'Transformer-11-MultiHeadSelfAttention',\n 'Transformer-11-MultiHeadSelfAttention-Dropout',\n 'Transformer-11-MultiHeadSelfAttention-Add',\n 'Transformer-11-MultiHeadSelfAttention-Norm',\n 'Transformer-11-FeedForward',\n 'Transformer-11-FeedForward-Dropout',\n 'Transformer-11-FeedForward-Add',\n 'Transformer-11-FeedForward-Norm']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.summary()\n",
    "names = [layer.name for layer in model.layers]\n",
    "names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-Token (None, None)\n",
      "Input-Segment (None, None)\n",
      "Embedding-Token (None, None, 768)\n",
      "Embedding-Segment (None, None, 768)\n",
      "Embedding-Token-Segment (None, None, 768)\n",
      "Embedding-Position (None, None, 768)\n",
      "Embedding-Norm (None, None, 768)\n",
      "Embedding-Dropout (None, None, 768)\n",
      "Transformer-0-MultiHeadSelfAttention (None, None, 768)\n",
      "Transformer-0-MultiHeadSelfAttention-Dropout (None, None, 768)\n",
      "Transformer-0-MultiHeadSelfAttention-Add (None, None, 768)\n",
      "Transformer-0-MultiHeadSelfAttention-Norm (None, None, 768)\n",
      "Transformer-0-FeedForward (None, None, 768)\n",
      "Transformer-0-FeedForward-Dropout (None, None, 768)\n",
      "Transformer-0-FeedForward-Add (None, None, 768)\n",
      "Transformer-0-FeedForward-Norm (None, None, 768)\n",
      "Transformer-1-MultiHeadSelfAttention (None, None, 768)\n",
      "Transformer-1-MultiHeadSelfAttention-Dropout (None, None, 768)\n",
      "Transformer-1-MultiHeadSelfAttention-Add (None, None, 768)\n",
      "Transformer-1-MultiHeadSelfAttention-Norm (None, None, 768)\n",
      "Transformer-1-FeedForward (None, None, 768)\n",
      "Transformer-1-FeedForward-Dropout (None, None, 768)\n",
      "Transformer-1-FeedForward-Add (None, None, 768)\n",
      "Transformer-1-FeedForward-Norm (None, None, 768)\n",
      "Transformer-2-MultiHeadSelfAttention (None, None, 768)\n",
      "Transformer-2-MultiHeadSelfAttention-Dropout (None, None, 768)\n",
      "Transformer-2-MultiHeadSelfAttention-Add (None, None, 768)\n",
      "Transformer-2-MultiHeadSelfAttention-Norm (None, None, 768)\n",
      "Transformer-2-FeedForward (None, None, 768)\n",
      "Transformer-2-FeedForward-Dropout (None, None, 768)\n",
      "Transformer-2-FeedForward-Add (None, None, 768)\n",
      "Transformer-2-FeedForward-Norm (None, None, 768)\n",
      "Transformer-3-MultiHeadSelfAttention (None, None, 768)\n",
      "Transformer-3-MultiHeadSelfAttention-Dropout (None, None, 768)\n",
      "Transformer-3-MultiHeadSelfAttention-Add (None, None, 768)\n",
      "Transformer-3-MultiHeadSelfAttention-Norm (None, None, 768)\n",
      "Transformer-3-FeedForward (None, None, 768)\n",
      "Transformer-3-FeedForward-Dropout (None, None, 768)\n",
      "Transformer-3-FeedForward-Add (None, None, 768)\n",
      "Transformer-3-FeedForward-Norm (None, None, 768)\n",
      "Transformer-4-MultiHeadSelfAttention (None, None, 768)\n",
      "Transformer-4-MultiHeadSelfAttention-Dropout (None, None, 768)\n",
      "Transformer-4-MultiHeadSelfAttention-Add (None, None, 768)\n",
      "Transformer-4-MultiHeadSelfAttention-Norm (None, None, 768)\n",
      "Transformer-4-FeedForward (None, None, 768)\n",
      "Transformer-4-FeedForward-Dropout (None, None, 768)\n",
      "Transformer-4-FeedForward-Add (None, None, 768)\n",
      "Transformer-4-FeedForward-Norm (None, None, 768)\n",
      "Transformer-5-MultiHeadSelfAttention (None, None, 768)\n",
      "Transformer-5-MultiHeadSelfAttention-Dropout (None, None, 768)\n",
      "Transformer-5-MultiHeadSelfAttention-Add (None, None, 768)\n",
      "Transformer-5-MultiHeadSelfAttention-Norm (None, None, 768)\n",
      "Transformer-5-FeedForward (None, None, 768)\n",
      "Transformer-5-FeedForward-Dropout (None, None, 768)\n",
      "Transformer-5-FeedForward-Add (None, None, 768)\n",
      "Transformer-5-FeedForward-Norm (None, None, 768)\n",
      "Transformer-6-MultiHeadSelfAttention (None, None, 768)\n",
      "Transformer-6-MultiHeadSelfAttention-Dropout (None, None, 768)\n",
      "Transformer-6-MultiHeadSelfAttention-Add (None, None, 768)\n",
      "Transformer-6-MultiHeadSelfAttention-Norm (None, None, 768)\n",
      "Transformer-6-FeedForward (None, None, 768)\n",
      "Transformer-6-FeedForward-Dropout (None, None, 768)\n",
      "Transformer-6-FeedForward-Add (None, None, 768)\n",
      "Transformer-6-FeedForward-Norm (None, None, 768)\n",
      "Transformer-7-MultiHeadSelfAttention (None, None, 768)\n",
      "Transformer-7-MultiHeadSelfAttention-Dropout (None, None, 768)\n",
      "Transformer-7-MultiHeadSelfAttention-Add (None, None, 768)\n",
      "Transformer-7-MultiHeadSelfAttention-Norm (None, None, 768)\n",
      "Transformer-7-FeedForward (None, None, 768)\n",
      "Transformer-7-FeedForward-Dropout (None, None, 768)\n",
      "Transformer-7-FeedForward-Add (None, None, 768)\n",
      "Transformer-7-FeedForward-Norm (None, None, 768)\n",
      "Transformer-8-MultiHeadSelfAttention (None, None, 768)\n",
      "Transformer-8-MultiHeadSelfAttention-Dropout (None, None, 768)\n",
      "Transformer-8-MultiHeadSelfAttention-Add (None, None, 768)\n",
      "Transformer-8-MultiHeadSelfAttention-Norm (None, None, 768)\n",
      "Transformer-8-FeedForward (None, None, 768)\n",
      "Transformer-8-FeedForward-Dropout (None, None, 768)\n",
      "Transformer-8-FeedForward-Add (None, None, 768)\n",
      "Transformer-8-FeedForward-Norm (None, None, 768)\n",
      "Transformer-9-MultiHeadSelfAttention (None, None, 768)\n",
      "Transformer-9-MultiHeadSelfAttention-Dropout (None, None, 768)\n",
      "Transformer-9-MultiHeadSelfAttention-Add (None, None, 768)\n",
      "Transformer-9-MultiHeadSelfAttention-Norm (None, None, 768)\n",
      "Transformer-9-FeedForward (None, None, 768)\n",
      "Transformer-9-FeedForward-Dropout (None, None, 768)\n",
      "Transformer-9-FeedForward-Add (None, None, 768)\n",
      "Transformer-9-FeedForward-Norm (None, None, 768)\n",
      "Transformer-10-MultiHeadSelfAttention (None, None, 768)\n",
      "Transformer-10-MultiHeadSelfAttention-Dropout (None, None, 768)\n",
      "Transformer-10-MultiHeadSelfAttention-Add (None, None, 768)\n",
      "Transformer-10-MultiHeadSelfAttention-Norm (None, None, 768)\n",
      "Transformer-10-FeedForward (None, None, 768)\n",
      "Transformer-10-FeedForward-Dropout (None, None, 768)\n",
      "Transformer-10-FeedForward-Add (None, None, 768)\n",
      "Transformer-10-FeedForward-Norm (None, None, 768)\n",
      "Transformer-11-MultiHeadSelfAttention (None, None, 768)\n",
      "Transformer-11-MultiHeadSelfAttention-Dropout (None, None, 768)\n",
      "Transformer-11-MultiHeadSelfAttention-Add (None, None, 768)\n",
      "Transformer-11-MultiHeadSelfAttention-Norm (None, None, 768)\n",
      "Transformer-11-FeedForward (None, None, 768)\n",
      "Transformer-11-FeedForward-Dropout (None, None, 768)\n",
      "Transformer-11-FeedForward-Add (None, None, 768)\n",
      "Transformer-11-FeedForward-Norm (None, None, 768)\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(model.layers)):\n",
    "    print(model.get_layer(index=index).name, model.get_layer(index=index).output_shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding-Token/embeddings:0 (21128, 768)\n",
      "Embedding-Segment/embeddings:0 (2, 768)\n",
      "Embedding-Position/embeddings:0 (512, 768)\n",
      "Embedding-Norm/beta:0 (768,)\n",
      "Embedding-Norm/gamma:0 (768,)\n",
      "Transformer-0-MultiHeadSelfAttention/dense_1/kernel:0 (768, 768)\n",
      "Transformer-0-MultiHeadSelfAttention/dense_1/bias:0 (768,)\n",
      "Transformer-0-MultiHeadSelfAttention/dense_2/kernel:0 (768, 768)\n",
      "Transformer-0-MultiHeadSelfAttention/dense_2/bias:0 (768,)\n",
      "Transformer-0-MultiHeadSelfAttention/dense_3/kernel:0 (768, 768)\n",
      "Transformer-0-MultiHeadSelfAttention/dense_3/bias:0 (768,)\n",
      "Transformer-0-MultiHeadSelfAttention/dense_4/kernel:0 (768, 768)\n",
      "Transformer-0-MultiHeadSelfAttention/dense_4/bias:0 (768,)\n",
      "Transformer-0-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-0-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Transformer-0-FeedForward/dense_5/kernel:0 (768, 3072)\n",
      "Transformer-0-FeedForward/dense_5/bias:0 (3072,)\n",
      "Transformer-0-FeedForward/dense_6/kernel:0 (3072, 768)\n",
      "Transformer-0-FeedForward/dense_6/bias:0 (768,)\n",
      "Transformer-0-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-0-FeedForward-Norm/gamma:0 (768,)\n",
      "Transformer-1-MultiHeadSelfAttention/dense_7/kernel:0 (768, 768)\n",
      "Transformer-1-MultiHeadSelfAttention/dense_7/bias:0 (768,)\n",
      "Transformer-1-MultiHeadSelfAttention/dense_8/kernel:0 (768, 768)\n",
      "Transformer-1-MultiHeadSelfAttention/dense_8/bias:0 (768,)\n",
      "Transformer-1-MultiHeadSelfAttention/dense_9/kernel:0 (768, 768)\n",
      "Transformer-1-MultiHeadSelfAttention/dense_9/bias:0 (768,)\n",
      "Transformer-1-MultiHeadSelfAttention/dense_10/kernel:0 (768, 768)\n",
      "Transformer-1-MultiHeadSelfAttention/dense_10/bias:0 (768,)\n",
      "Transformer-1-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-1-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Transformer-1-FeedForward/dense_11/kernel:0 (768, 3072)\n",
      "Transformer-1-FeedForward/dense_11/bias:0 (3072,)\n",
      "Transformer-1-FeedForward/dense_12/kernel:0 (3072, 768)\n",
      "Transformer-1-FeedForward/dense_12/bias:0 (768,)\n",
      "Transformer-1-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-1-FeedForward-Norm/gamma:0 (768,)\n",
      "Transformer-2-MultiHeadSelfAttention/dense_13/kernel:0 (768, 768)\n",
      "Transformer-2-MultiHeadSelfAttention/dense_13/bias:0 (768,)\n",
      "Transformer-2-MultiHeadSelfAttention/dense_14/kernel:0 (768, 768)\n",
      "Transformer-2-MultiHeadSelfAttention/dense_14/bias:0 (768,)\n",
      "Transformer-2-MultiHeadSelfAttention/dense_15/kernel:0 (768, 768)\n",
      "Transformer-2-MultiHeadSelfAttention/dense_15/bias:0 (768,)\n",
      "Transformer-2-MultiHeadSelfAttention/dense_16/kernel:0 (768, 768)\n",
      "Transformer-2-MultiHeadSelfAttention/dense_16/bias:0 (768,)\n",
      "Transformer-2-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-2-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Transformer-2-FeedForward/dense_17/kernel:0 (768, 3072)\n",
      "Transformer-2-FeedForward/dense_17/bias:0 (3072,)\n",
      "Transformer-2-FeedForward/dense_18/kernel:0 (3072, 768)\n",
      "Transformer-2-FeedForward/dense_18/bias:0 (768,)\n",
      "Transformer-2-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-2-FeedForward-Norm/gamma:0 (768,)\n",
      "Transformer-3-MultiHeadSelfAttention/dense_19/kernel:0 (768, 768)\n",
      "Transformer-3-MultiHeadSelfAttention/dense_19/bias:0 (768,)\n",
      "Transformer-3-MultiHeadSelfAttention/dense_20/kernel:0 (768, 768)\n",
      "Transformer-3-MultiHeadSelfAttention/dense_20/bias:0 (768,)\n",
      "Transformer-3-MultiHeadSelfAttention/dense_21/kernel:0 (768, 768)\n",
      "Transformer-3-MultiHeadSelfAttention/dense_21/bias:0 (768,)\n",
      "Transformer-3-MultiHeadSelfAttention/dense_22/kernel:0 (768, 768)\n",
      "Transformer-3-MultiHeadSelfAttention/dense_22/bias:0 (768,)\n",
      "Transformer-3-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-3-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Transformer-3-FeedForward/dense_23/kernel:0 (768, 3072)\n",
      "Transformer-3-FeedForward/dense_23/bias:0 (3072,)\n",
      "Transformer-3-FeedForward/dense_24/kernel:0 (3072, 768)\n",
      "Transformer-3-FeedForward/dense_24/bias:0 (768,)\n",
      "Transformer-3-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-3-FeedForward-Norm/gamma:0 (768,)\n",
      "Transformer-4-MultiHeadSelfAttention/dense_25/kernel:0 (768, 768)\n",
      "Transformer-4-MultiHeadSelfAttention/dense_25/bias:0 (768,)\n",
      "Transformer-4-MultiHeadSelfAttention/dense_26/kernel:0 (768, 768)\n",
      "Transformer-4-MultiHeadSelfAttention/dense_26/bias:0 (768,)\n",
      "Transformer-4-MultiHeadSelfAttention/dense_27/kernel:0 (768, 768)\n",
      "Transformer-4-MultiHeadSelfAttention/dense_27/bias:0 (768,)\n",
      "Transformer-4-MultiHeadSelfAttention/dense_28/kernel:0 (768, 768)\n",
      "Transformer-4-MultiHeadSelfAttention/dense_28/bias:0 (768,)\n",
      "Transformer-4-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-4-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Transformer-4-FeedForward/dense_29/kernel:0 (768, 3072)\n",
      "Transformer-4-FeedForward/dense_29/bias:0 (3072,)\n",
      "Transformer-4-FeedForward/dense_30/kernel:0 (3072, 768)\n",
      "Transformer-4-FeedForward/dense_30/bias:0 (768,)\n",
      "Transformer-4-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-4-FeedForward-Norm/gamma:0 (768,)\n",
      "Transformer-5-MultiHeadSelfAttention/dense_31/kernel:0 (768, 768)\n",
      "Transformer-5-MultiHeadSelfAttention/dense_31/bias:0 (768,)\n",
      "Transformer-5-MultiHeadSelfAttention/dense_32/kernel:0 (768, 768)\n",
      "Transformer-5-MultiHeadSelfAttention/dense_32/bias:0 (768,)\n",
      "Transformer-5-MultiHeadSelfAttention/dense_33/kernel:0 (768, 768)\n",
      "Transformer-5-MultiHeadSelfAttention/dense_33/bias:0 (768,)\n",
      "Transformer-5-MultiHeadSelfAttention/dense_34/kernel:0 (768, 768)\n",
      "Transformer-5-MultiHeadSelfAttention/dense_34/bias:0 (768,)\n",
      "Transformer-5-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-5-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Transformer-5-FeedForward/dense_35/kernel:0 (768, 3072)\n",
      "Transformer-5-FeedForward/dense_35/bias:0 (3072,)\n",
      "Transformer-5-FeedForward/dense_36/kernel:0 (3072, 768)\n",
      "Transformer-5-FeedForward/dense_36/bias:0 (768,)\n",
      "Transformer-5-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-5-FeedForward-Norm/gamma:0 (768,)\n",
      "Transformer-6-MultiHeadSelfAttention/dense_37/kernel:0 (768, 768)\n",
      "Transformer-6-MultiHeadSelfAttention/dense_37/bias:0 (768,)\n",
      "Transformer-6-MultiHeadSelfAttention/dense_38/kernel:0 (768, 768)\n",
      "Transformer-6-MultiHeadSelfAttention/dense_38/bias:0 (768,)\n",
      "Transformer-6-MultiHeadSelfAttention/dense_39/kernel:0 (768, 768)\n",
      "Transformer-6-MultiHeadSelfAttention/dense_39/bias:0 (768,)\n",
      "Transformer-6-MultiHeadSelfAttention/dense_40/kernel:0 (768, 768)\n",
      "Transformer-6-MultiHeadSelfAttention/dense_40/bias:0 (768,)\n",
      "Transformer-6-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-6-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Transformer-6-FeedForward/dense_41/kernel:0 (768, 3072)\n",
      "Transformer-6-FeedForward/dense_41/bias:0 (3072,)\n",
      "Transformer-6-FeedForward/dense_42/kernel:0 (3072, 768)\n",
      "Transformer-6-FeedForward/dense_42/bias:0 (768,)\n",
      "Transformer-6-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-6-FeedForward-Norm/gamma:0 (768,)\n",
      "Transformer-7-MultiHeadSelfAttention/dense_43/kernel:0 (768, 768)\n",
      "Transformer-7-MultiHeadSelfAttention/dense_43/bias:0 (768,)\n",
      "Transformer-7-MultiHeadSelfAttention/dense_44/kernel:0 (768, 768)\n",
      "Transformer-7-MultiHeadSelfAttention/dense_44/bias:0 (768,)\n",
      "Transformer-7-MultiHeadSelfAttention/dense_45/kernel:0 (768, 768)\n",
      "Transformer-7-MultiHeadSelfAttention/dense_45/bias:0 (768,)\n",
      "Transformer-7-MultiHeadSelfAttention/dense_46/kernel:0 (768, 768)\n",
      "Transformer-7-MultiHeadSelfAttention/dense_46/bias:0 (768,)\n",
      "Transformer-7-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-7-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Transformer-7-FeedForward/dense_47/kernel:0 (768, 3072)\n",
      "Transformer-7-FeedForward/dense_47/bias:0 (3072,)\n",
      "Transformer-7-FeedForward/dense_48/kernel:0 (3072, 768)\n",
      "Transformer-7-FeedForward/dense_48/bias:0 (768,)\n",
      "Transformer-7-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-7-FeedForward-Norm/gamma:0 (768,)\n",
      "Transformer-8-MultiHeadSelfAttention/dense_49/kernel:0 (768, 768)\n",
      "Transformer-8-MultiHeadSelfAttention/dense_49/bias:0 (768,)\n",
      "Transformer-8-MultiHeadSelfAttention/dense_50/kernel:0 (768, 768)\n",
      "Transformer-8-MultiHeadSelfAttention/dense_50/bias:0 (768,)\n",
      "Transformer-8-MultiHeadSelfAttention/dense_51/kernel:0 (768, 768)\n",
      "Transformer-8-MultiHeadSelfAttention/dense_51/bias:0 (768,)\n",
      "Transformer-8-MultiHeadSelfAttention/dense_52/kernel:0 (768, 768)\n",
      "Transformer-8-MultiHeadSelfAttention/dense_52/bias:0 (768,)\n",
      "Transformer-8-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-8-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Transformer-8-FeedForward/dense_53/kernel:0 (768, 3072)\n",
      "Transformer-8-FeedForward/dense_53/bias:0 (3072,)\n",
      "Transformer-8-FeedForward/dense_54/kernel:0 (3072, 768)\n",
      "Transformer-8-FeedForward/dense_54/bias:0 (768,)\n",
      "Transformer-8-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-8-FeedForward-Norm/gamma:0 (768,)\n",
      "Transformer-9-MultiHeadSelfAttention/dense_55/kernel:0 (768, 768)\n",
      "Transformer-9-MultiHeadSelfAttention/dense_55/bias:0 (768,)\n",
      "Transformer-9-MultiHeadSelfAttention/dense_56/kernel:0 (768, 768)\n",
      "Transformer-9-MultiHeadSelfAttention/dense_56/bias:0 (768,)\n",
      "Transformer-9-MultiHeadSelfAttention/dense_57/kernel:0 (768, 768)\n",
      "Transformer-9-MultiHeadSelfAttention/dense_57/bias:0 (768,)\n",
      "Transformer-9-MultiHeadSelfAttention/dense_58/kernel:0 (768, 768)\n",
      "Transformer-9-MultiHeadSelfAttention/dense_58/bias:0 (768,)\n",
      "Transformer-9-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-9-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Transformer-9-FeedForward/dense_59/kernel:0 (768, 3072)\n",
      "Transformer-9-FeedForward/dense_59/bias:0 (3072,)\n",
      "Transformer-9-FeedForward/dense_60/kernel:0 (3072, 768)\n",
      "Transformer-9-FeedForward/dense_60/bias:0 (768,)\n",
      "Transformer-9-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-9-FeedForward-Norm/gamma:0 (768,)\n",
      "Transformer-10-MultiHeadSelfAttention/dense_61/kernel:0 (768, 768)\n",
      "Transformer-10-MultiHeadSelfAttention/dense_61/bias:0 (768,)\n",
      "Transformer-10-MultiHeadSelfAttention/dense_62/kernel:0 (768, 768)\n",
      "Transformer-10-MultiHeadSelfAttention/dense_62/bias:0 (768,)\n",
      "Transformer-10-MultiHeadSelfAttention/dense_63/kernel:0 (768, 768)\n",
      "Transformer-10-MultiHeadSelfAttention/dense_63/bias:0 (768,)\n",
      "Transformer-10-MultiHeadSelfAttention/dense_64/kernel:0 (768, 768)\n",
      "Transformer-10-MultiHeadSelfAttention/dense_64/bias:0 (768,)\n",
      "Transformer-10-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-10-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Transformer-10-FeedForward/dense_65/kernel:0 (768, 3072)\n",
      "Transformer-10-FeedForward/dense_65/bias:0 (3072,)\n",
      "Transformer-10-FeedForward/dense_66/kernel:0 (3072, 768)\n",
      "Transformer-10-FeedForward/dense_66/bias:0 (768,)\n",
      "Transformer-10-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-10-FeedForward-Norm/gamma:0 (768,)\n",
      "Transformer-11-MultiHeadSelfAttention/dense_67/kernel:0 (768, 768)\n",
      "Transformer-11-MultiHeadSelfAttention/dense_67/bias:0 (768,)\n",
      "Transformer-11-MultiHeadSelfAttention/dense_68/kernel:0 (768, 768)\n",
      "Transformer-11-MultiHeadSelfAttention/dense_68/bias:0 (768,)\n",
      "Transformer-11-MultiHeadSelfAttention/dense_69/kernel:0 (768, 768)\n",
      "Transformer-11-MultiHeadSelfAttention/dense_69/bias:0 (768,)\n",
      "Transformer-11-MultiHeadSelfAttention/dense_70/kernel:0 (768, 768)\n",
      "Transformer-11-MultiHeadSelfAttention/dense_70/bias:0 (768,)\n",
      "Transformer-11-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-11-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "Transformer-11-FeedForward/dense_71/kernel:0 (768, 3072)\n",
      "Transformer-11-FeedForward/dense_71/bias:0 (3072,)\n",
      "Transformer-11-FeedForward/dense_72/kernel:0 (3072, 768)\n",
      "Transformer-11-FeedForward/dense_72/bias:0 (768,)\n",
      "Transformer-11-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-11-FeedForward-Norm/gamma:0 (768,)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    for weight in layer.weights:\n",
    "        print(weight.name, weight.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class Attention(tf.keras.Model):\n",
    "    \"\"\"keras风格的广义attention\"\"\"\n",
    "    def __init__(self, dim=768, n_head=12, head_dim=64, initializer_range=0.2):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            dim: cross attention变量的feature维度\n",
    "            n_head: multi-head attention的head个数\n",
    "            head_dim: 每个head的维度\n",
    "            initializer_range: 全连接层参数初始化参数\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = head_dim\n",
    "        self.initializer_range = initializer_range\n",
    "        self.inner_dim = self.n_head * self.head_dim  # inner_dim是n_head * head_dim之后将要被reshape为n_head和head_dim\n",
    "        self.scale = self.head_dim ** -0.5  # scale是1/根号下dk\n",
    "        self.query_layer = tf.keras.layers.Dense(\n",
    "            units=self.inner_dim,\n",
    "            # use_bias=False,\n",
    "            name='query',\n",
    "            kernel_initializer=modeling.create_initializer(initializer_range)\n",
    "        )\n",
    "        self.key_layer = tf.keras.layers.Dense(\n",
    "            units=self.inner_dim,\n",
    "            # use_bias=False,\n",
    "            name='key',\n",
    "            kernel_initializer=modeling.create_initializer(initializer_range)\n",
    "        )\n",
    "        self.value_layer = tf.keras.layers.Dense(\n",
    "            units=self.inner_dim,\n",
    "            # use_bias=False,\n",
    "            name='value',\n",
    "            kernel_initializer=modeling.create_initializer(initializer_range)\n",
    "        )\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            units=self.dim,\n",
    "            # use_bias=False,\n",
    "            name='output',\n",
    "            kernel_initializer=modeling.create_initializer(initializer_range)\n",
    "        )\n",
    "\n",
    "    def transpose_for_scores(self, input_tensor, batch_size, n_head, seq_length, head_dim):\n",
    "        \"\"\"\n",
    "        该方法对对input_tensor进行reshape\n",
    "        input_tensor [batch, seq_len, dim] -> [batch, seq_len, n_head, head_dim] -> [batch, n_head, seq_len, head_dim]\n",
    "        \"\"\"\n",
    "        output_tensor = tf.reshape(\n",
    "            input_tensor, [batch_size, seq_length, n_head, head_dim])\n",
    "        output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n",
    "        return output_tensor\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        \"\"\"\n",
    "        输入:\n",
    "            from_tensor 用于query的tensor [batch, seq_len_from, dim]\n",
    "            to_tensor 用于key和value的tensor [batch, seq_len_to, dim]\n",
    "            若from_tensor == to_tensor 即为self-attention 否则为cross-attention\n",
    "            attention_mask 用于对attention加以mask [batch, seq_len_from, seq_len_to] 值为0或1 对0位置进行mask 对1位置进行保留\n",
    "        输出:\n",
    "            output 经过广义attention得到的结果 [batch, seq_len_from, n_head*head_dim]\n",
    "        \"\"\"\n",
    "        from_tensor, to_tensor = inputs[:2]\n",
    "        from_shape = modeling.get_shape_list(from_tensor, expected_rank=3)\n",
    "        to_shape = modeling.get_shape_list(to_tensor, expected_rank=3)\n",
    "        # assert from_shape[0] == to_shape[0]\n",
    "        # assert from_shape[-1] == to_shape[-1] == self.dim\n",
    "        batch = from_shape[0]\n",
    "        seq_len_from = from_shape[1]\n",
    "        seq_len_to = to_shape[1]\n",
    "        # 计算维度\n",
    "\n",
    "        query = self.query_layer(from_tensor)\n",
    "        # query [batch, seq_len_from, inner_dim]\n",
    "        key = self.key_layer(to_tensor)\n",
    "        # key [batch, seq_len_to, inner_dim]\n",
    "        value = self.value_layer(to_tensor)\n",
    "        # value [batch, seq_len_to, inner_dim]\n",
    "\n",
    "        query = self.transpose_for_scores(query, batch, self.n_head, seq_len_from, self.head_dim)\n",
    "        key = self.transpose_for_scores(key, batch, self.n_head, seq_len_to, self.head_dim)\n",
    "        value = self.transpose_for_scores(value, batch, self.n_head, seq_len_to, self.head_dim)\n",
    "        # query [batch, n_head, seq_len_from, head_dim]\n",
    "        # key value [batch, n_head, seq_len_to, head_dim]\n",
    "\n",
    "        query = query * self.scale\n",
    "        # query = query / 根号下dk\n",
    "\n",
    "        attention_score = tf.einsum('...ik,...jk->...ij', query, key)\n",
    "        # i->seq_len_from j->seq_len_to k->head_dim\n",
    "        # attention_score [batch, n_head, seq_len_from, seq_len_to]\n",
    "\n",
    "        if len(inputs) == 3:\n",
    "            attention_mask = inputs[2]\n",
    "            # [batch, seq_len_from, seq_len_to]\n",
    "            attention_mask = tf.expand_dims(attention_mask, axis=[1])\n",
    "            # [batch, 1, seq_len_from, seq_len_to]\n",
    "            adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n",
    "            # 对于0进行mask对于1不mask 所以(1-mask_val) * -10000.0 0->-10000.0 1->0.0\n",
    "            attention_score += adder\n",
    "            # attention_score加上adder就是mask\n",
    "\n",
    "        attention_probs = tf.nn.softmax(attention_score)\n",
    "        # attention_score经过softmax得到概率分布\n",
    "        # attention_probs [batch, n_head, seq_len_from, seq_len_to]\n",
    "\n",
    "        output = tf.einsum('...ij,...jk->...ik', attention_probs, value)\n",
    "        # i->seq_len_from j->seq_len_to k->head_dim\n",
    "        # output [batch, n_head, seq_len_from, head_dim]\n",
    "\n",
    "        output = tf.transpose(output, [0, 2, 1, 3])\n",
    "        output = tf.reshape(output, shape=(batch, seq_len_from, self.inner_dim))\n",
    "        output = self.output_layer(output)\n",
    "        # [batch, seq_len_from, dim]\n",
    "        return output\n",
    "\n",
    "\n",
    "class LMBlock(tf.keras.Model):\n",
    "    \"\"\"LM Block 本质为self-attention + ffw\"\"\"\n",
    "    def __init__(self, dim=768, n_head=12, ffw_mult=4):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            dim: cross attention变量的feature维度\n",
    "            n_head: multi-head attention的head个数\n",
    "            ffw_mult: feedforward的参数量相对于feature的倍数 经验值为4\n",
    "        \"\"\"\n",
    "        super(LMBlock, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.n_head = n_head\n",
    "        self.ffw_mult = ffw_mult\n",
    "        if self.dim % n_head != 0:\n",
    "            raise ValueError(\n",
    "                \"参数dim必须整除n_head\"\n",
    "            )\n",
    "        self.head_dim = self.dim // self.n_head\n",
    "        self.all_layers = list()\n",
    "        self.attn = Attention(dim=self.dim, n_head=self.n_head, head_dim=self.head_dim)\n",
    "        self.norm = tf.keras.layers.LayerNormalization()\n",
    "        self.ffw = utils.FeedForward(dim=self.dim, mult=self.ffw_mult)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        \"\"\"\n",
    "        输入:\n",
    "            input_tensor: 输入的input_tensor [batch, seq_len, dim]\n",
    "            目前LMBlock实现中 并没有mask 原因在于Flamingo所对应的场景不需要进行mask\n",
    "        输出:\n",
    "            output: 经过LM计算后的结果 [batch, seq_len, dim]\n",
    "        \"\"\"\n",
    "        input_tensor = inputs\n",
    "        input_shape = modeling.get_shape_list(input_tensor, expected_rank=3)\n",
    "        assert self.dim == input_shape[2]\n",
    "\n",
    "        # input_tensor = self.pre_norm(input_tensor)\n",
    "        input_tensor = input_tensor + self.attn(inputs=(input_tensor, input_tensor))\n",
    "        # self_attention + residual\n",
    "        input_tensor = self.norm(input_tensor)\n",
    "        # attn之后layer norm\n",
    "        input_tensor = input_tensor + self.ffw(inputs=input_tensor)\n",
    "        # ffw + residual\n",
    "        return input_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "lm_block = LMBlock()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "lm_block.build(input_shape=(None, None, 768))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([Dimension(32), Dimension(64), Dimension(768)])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = tf.random.normal(shape=(32, 64, 768))\n",
    "output = lm_block(input_tensor)\n",
    "output.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lm_block_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_normalization_6 (Layer multiple                  1536      \n",
      "_________________________________________________________________\n",
      "attention_3 (Attention)      multiple                  2362368   \n",
      "_________________________________________________________________\n",
      "feed_forward_3 (FeedForward) multiple                  4723968   \n",
      "=================================================================\n",
      "Total params: 7,087,872\n",
      "Trainable params: 7,087,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lm_block.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "layer name attention_2\n",
      "attention_2/query/kernel:0 (768, 768)\n",
      "attention_2/query/bias:0 (768,)\n",
      "attention_2/key/kernel:0 (768, 768)\n",
      "attention_2/key/bias:0 (768,)\n",
      "attention_2/value/kernel:0 (768, 768)\n",
      "attention_2/value/bias:0 (768,)\n",
      "attention_2/output/kernel:0 (768, 768)\n",
      "attention_2/output/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name layer_normalization_4\n",
      "layer_normalization_4/gamma:0 (768,)\n",
      "layer_normalization_4/beta:0 (768,)\n",
      "-------------------------\n",
      "layer name feed_forward_2\n",
      "feed_forward_2/dense_4/kernel:0 (768, 3072)\n",
      "feed_forward_2/dense_4/bias:0 (3072,)\n",
      "feed_forward_2/dense_5/kernel:0 (3072, 768)\n",
      "feed_forward_2/dense_5/bias:0 (768,)\n",
      "feed_forward_2/layer_normalization_5/gamma:0 (768,)\n",
      "feed_forward_2/layer_normalization_5/beta:0 (768,)\n"
     ]
    }
   ],
   "source": [
    "for layer in lm_block.layers:\n",
    "    print('-------------------------')\n",
    "    print(f'layer name {layer.name}')\n",
    "    for weight in layer.weights:\n",
    "        print(weight.name, weight.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "layer name Input-Token\n",
      "-------------------------\n",
      "layer name Input-Segment\n",
      "-------------------------\n",
      "layer name Embedding-Token\n",
      "Embedding-Token/embeddings:0 (21128, 768)\n",
      "-------------------------\n",
      "layer name Embedding-Segment\n",
      "Embedding-Segment/embeddings:0 (2, 768)\n",
      "-------------------------\n",
      "layer name Embedding-Token-Segment\n",
      "-------------------------\n",
      "layer name Embedding-Position\n",
      "Embedding-Position/embeddings:0 (512, 768)\n",
      "-------------------------\n",
      "layer name Embedding-Norm\n",
      "Embedding-Norm/beta:0 (768,)\n",
      "Embedding-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Embedding-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-0-MultiHeadSelfAttention\n",
      "Transformer-0-MultiHeadSelfAttention/dense_1/kernel:0 (768, 768)\n",
      "Transformer-0-MultiHeadSelfAttention/dense_1/bias:0 (768,)\n",
      "Transformer-0-MultiHeadSelfAttention/dense_2/kernel:0 (768, 768)\n",
      "Transformer-0-MultiHeadSelfAttention/dense_2/bias:0 (768,)\n",
      "Transformer-0-MultiHeadSelfAttention/dense_3/kernel:0 (768, 768)\n",
      "Transformer-0-MultiHeadSelfAttention/dense_3/bias:0 (768,)\n",
      "Transformer-0-MultiHeadSelfAttention/dense_4/kernel:0 (768, 768)\n",
      "Transformer-0-MultiHeadSelfAttention/dense_4/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-0-MultiHeadSelfAttention-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-0-MultiHeadSelfAttention-Add\n",
      "-------------------------\n",
      "layer name Transformer-0-MultiHeadSelfAttention-Norm\n",
      "Transformer-0-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-0-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-0-FeedForward\n",
      "Transformer-0-FeedForward/dense_5/kernel:0 (768, 3072)\n",
      "Transformer-0-FeedForward/dense_5/bias:0 (3072,)\n",
      "Transformer-0-FeedForward/dense_6/kernel:0 (3072, 768)\n",
      "Transformer-0-FeedForward/dense_6/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-0-FeedForward-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-0-FeedForward-Add\n",
      "-------------------------\n",
      "layer name Transformer-0-FeedForward-Norm\n",
      "Transformer-0-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-0-FeedForward-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-1-MultiHeadSelfAttention\n",
      "Transformer-1-MultiHeadSelfAttention/dense_7/kernel:0 (768, 768)\n",
      "Transformer-1-MultiHeadSelfAttention/dense_7/bias:0 (768,)\n",
      "Transformer-1-MultiHeadSelfAttention/dense_8/kernel:0 (768, 768)\n",
      "Transformer-1-MultiHeadSelfAttention/dense_8/bias:0 (768,)\n",
      "Transformer-1-MultiHeadSelfAttention/dense_9/kernel:0 (768, 768)\n",
      "Transformer-1-MultiHeadSelfAttention/dense_9/bias:0 (768,)\n",
      "Transformer-1-MultiHeadSelfAttention/dense_10/kernel:0 (768, 768)\n",
      "Transformer-1-MultiHeadSelfAttention/dense_10/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-1-MultiHeadSelfAttention-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-1-MultiHeadSelfAttention-Add\n",
      "-------------------------\n",
      "layer name Transformer-1-MultiHeadSelfAttention-Norm\n",
      "Transformer-1-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-1-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-1-FeedForward\n",
      "Transformer-1-FeedForward/dense_11/kernel:0 (768, 3072)\n",
      "Transformer-1-FeedForward/dense_11/bias:0 (3072,)\n",
      "Transformer-1-FeedForward/dense_12/kernel:0 (3072, 768)\n",
      "Transformer-1-FeedForward/dense_12/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-1-FeedForward-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-1-FeedForward-Add\n",
      "-------------------------\n",
      "layer name Transformer-1-FeedForward-Norm\n",
      "Transformer-1-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-1-FeedForward-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-2-MultiHeadSelfAttention\n",
      "Transformer-2-MultiHeadSelfAttention/dense_13/kernel:0 (768, 768)\n",
      "Transformer-2-MultiHeadSelfAttention/dense_13/bias:0 (768,)\n",
      "Transformer-2-MultiHeadSelfAttention/dense_14/kernel:0 (768, 768)\n",
      "Transformer-2-MultiHeadSelfAttention/dense_14/bias:0 (768,)\n",
      "Transformer-2-MultiHeadSelfAttention/dense_15/kernel:0 (768, 768)\n",
      "Transformer-2-MultiHeadSelfAttention/dense_15/bias:0 (768,)\n",
      "Transformer-2-MultiHeadSelfAttention/dense_16/kernel:0 (768, 768)\n",
      "Transformer-2-MultiHeadSelfAttention/dense_16/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-2-MultiHeadSelfAttention-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-2-MultiHeadSelfAttention-Add\n",
      "-------------------------\n",
      "layer name Transformer-2-MultiHeadSelfAttention-Norm\n",
      "Transformer-2-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-2-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-2-FeedForward\n",
      "Transformer-2-FeedForward/dense_17/kernel:0 (768, 3072)\n",
      "Transformer-2-FeedForward/dense_17/bias:0 (3072,)\n",
      "Transformer-2-FeedForward/dense_18/kernel:0 (3072, 768)\n",
      "Transformer-2-FeedForward/dense_18/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-2-FeedForward-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-2-FeedForward-Add\n",
      "-------------------------\n",
      "layer name Transformer-2-FeedForward-Norm\n",
      "Transformer-2-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-2-FeedForward-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-3-MultiHeadSelfAttention\n",
      "Transformer-3-MultiHeadSelfAttention/dense_19/kernel:0 (768, 768)\n",
      "Transformer-3-MultiHeadSelfAttention/dense_19/bias:0 (768,)\n",
      "Transformer-3-MultiHeadSelfAttention/dense_20/kernel:0 (768, 768)\n",
      "Transformer-3-MultiHeadSelfAttention/dense_20/bias:0 (768,)\n",
      "Transformer-3-MultiHeadSelfAttention/dense_21/kernel:0 (768, 768)\n",
      "Transformer-3-MultiHeadSelfAttention/dense_21/bias:0 (768,)\n",
      "Transformer-3-MultiHeadSelfAttention/dense_22/kernel:0 (768, 768)\n",
      "Transformer-3-MultiHeadSelfAttention/dense_22/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-3-MultiHeadSelfAttention-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-3-MultiHeadSelfAttention-Add\n",
      "-------------------------\n",
      "layer name Transformer-3-MultiHeadSelfAttention-Norm\n",
      "Transformer-3-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-3-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-3-FeedForward\n",
      "Transformer-3-FeedForward/dense_23/kernel:0 (768, 3072)\n",
      "Transformer-3-FeedForward/dense_23/bias:0 (3072,)\n",
      "Transformer-3-FeedForward/dense_24/kernel:0 (3072, 768)\n",
      "Transformer-3-FeedForward/dense_24/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-3-FeedForward-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-3-FeedForward-Add\n",
      "-------------------------\n",
      "layer name Transformer-3-FeedForward-Norm\n",
      "Transformer-3-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-3-FeedForward-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-4-MultiHeadSelfAttention\n",
      "Transformer-4-MultiHeadSelfAttention/dense_25/kernel:0 (768, 768)\n",
      "Transformer-4-MultiHeadSelfAttention/dense_25/bias:0 (768,)\n",
      "Transformer-4-MultiHeadSelfAttention/dense_26/kernel:0 (768, 768)\n",
      "Transformer-4-MultiHeadSelfAttention/dense_26/bias:0 (768,)\n",
      "Transformer-4-MultiHeadSelfAttention/dense_27/kernel:0 (768, 768)\n",
      "Transformer-4-MultiHeadSelfAttention/dense_27/bias:0 (768,)\n",
      "Transformer-4-MultiHeadSelfAttention/dense_28/kernel:0 (768, 768)\n",
      "Transformer-4-MultiHeadSelfAttention/dense_28/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-4-MultiHeadSelfAttention-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-4-MultiHeadSelfAttention-Add\n",
      "-------------------------\n",
      "layer name Transformer-4-MultiHeadSelfAttention-Norm\n",
      "Transformer-4-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-4-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-4-FeedForward\n",
      "Transformer-4-FeedForward/dense_29/kernel:0 (768, 3072)\n",
      "Transformer-4-FeedForward/dense_29/bias:0 (3072,)\n",
      "Transformer-4-FeedForward/dense_30/kernel:0 (3072, 768)\n",
      "Transformer-4-FeedForward/dense_30/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-4-FeedForward-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-4-FeedForward-Add\n",
      "-------------------------\n",
      "layer name Transformer-4-FeedForward-Norm\n",
      "Transformer-4-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-4-FeedForward-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-5-MultiHeadSelfAttention\n",
      "Transformer-5-MultiHeadSelfAttention/dense_31/kernel:0 (768, 768)\n",
      "Transformer-5-MultiHeadSelfAttention/dense_31/bias:0 (768,)\n",
      "Transformer-5-MultiHeadSelfAttention/dense_32/kernel:0 (768, 768)\n",
      "Transformer-5-MultiHeadSelfAttention/dense_32/bias:0 (768,)\n",
      "Transformer-5-MultiHeadSelfAttention/dense_33/kernel:0 (768, 768)\n",
      "Transformer-5-MultiHeadSelfAttention/dense_33/bias:0 (768,)\n",
      "Transformer-5-MultiHeadSelfAttention/dense_34/kernel:0 (768, 768)\n",
      "Transformer-5-MultiHeadSelfAttention/dense_34/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-5-MultiHeadSelfAttention-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-5-MultiHeadSelfAttention-Add\n",
      "-------------------------\n",
      "layer name Transformer-5-MultiHeadSelfAttention-Norm\n",
      "Transformer-5-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-5-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-5-FeedForward\n",
      "Transformer-5-FeedForward/dense_35/kernel:0 (768, 3072)\n",
      "Transformer-5-FeedForward/dense_35/bias:0 (3072,)\n",
      "Transformer-5-FeedForward/dense_36/kernel:0 (3072, 768)\n",
      "Transformer-5-FeedForward/dense_36/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-5-FeedForward-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-5-FeedForward-Add\n",
      "-------------------------\n",
      "layer name Transformer-5-FeedForward-Norm\n",
      "Transformer-5-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-5-FeedForward-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-6-MultiHeadSelfAttention\n",
      "Transformer-6-MultiHeadSelfAttention/dense_37/kernel:0 (768, 768)\n",
      "Transformer-6-MultiHeadSelfAttention/dense_37/bias:0 (768,)\n",
      "Transformer-6-MultiHeadSelfAttention/dense_38/kernel:0 (768, 768)\n",
      "Transformer-6-MultiHeadSelfAttention/dense_38/bias:0 (768,)\n",
      "Transformer-6-MultiHeadSelfAttention/dense_39/kernel:0 (768, 768)\n",
      "Transformer-6-MultiHeadSelfAttention/dense_39/bias:0 (768,)\n",
      "Transformer-6-MultiHeadSelfAttention/dense_40/kernel:0 (768, 768)\n",
      "Transformer-6-MultiHeadSelfAttention/dense_40/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-6-MultiHeadSelfAttention-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-6-MultiHeadSelfAttention-Add\n",
      "-------------------------\n",
      "layer name Transformer-6-MultiHeadSelfAttention-Norm\n",
      "Transformer-6-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-6-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-6-FeedForward\n",
      "Transformer-6-FeedForward/dense_41/kernel:0 (768, 3072)\n",
      "Transformer-6-FeedForward/dense_41/bias:0 (3072,)\n",
      "Transformer-6-FeedForward/dense_42/kernel:0 (3072, 768)\n",
      "Transformer-6-FeedForward/dense_42/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-6-FeedForward-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-6-FeedForward-Add\n",
      "-------------------------\n",
      "layer name Transformer-6-FeedForward-Norm\n",
      "Transformer-6-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-6-FeedForward-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-7-MultiHeadSelfAttention\n",
      "Transformer-7-MultiHeadSelfAttention/dense_43/kernel:0 (768, 768)\n",
      "Transformer-7-MultiHeadSelfAttention/dense_43/bias:0 (768,)\n",
      "Transformer-7-MultiHeadSelfAttention/dense_44/kernel:0 (768, 768)\n",
      "Transformer-7-MultiHeadSelfAttention/dense_44/bias:0 (768,)\n",
      "Transformer-7-MultiHeadSelfAttention/dense_45/kernel:0 (768, 768)\n",
      "Transformer-7-MultiHeadSelfAttention/dense_45/bias:0 (768,)\n",
      "Transformer-7-MultiHeadSelfAttention/dense_46/kernel:0 (768, 768)\n",
      "Transformer-7-MultiHeadSelfAttention/dense_46/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-7-MultiHeadSelfAttention-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-7-MultiHeadSelfAttention-Add\n",
      "-------------------------\n",
      "layer name Transformer-7-MultiHeadSelfAttention-Norm\n",
      "Transformer-7-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-7-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-7-FeedForward\n",
      "Transformer-7-FeedForward/dense_47/kernel:0 (768, 3072)\n",
      "Transformer-7-FeedForward/dense_47/bias:0 (3072,)\n",
      "Transformer-7-FeedForward/dense_48/kernel:0 (3072, 768)\n",
      "Transformer-7-FeedForward/dense_48/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-7-FeedForward-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-7-FeedForward-Add\n",
      "-------------------------\n",
      "layer name Transformer-7-FeedForward-Norm\n",
      "Transformer-7-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-7-FeedForward-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-8-MultiHeadSelfAttention\n",
      "Transformer-8-MultiHeadSelfAttention/dense_49/kernel:0 (768, 768)\n",
      "Transformer-8-MultiHeadSelfAttention/dense_49/bias:0 (768,)\n",
      "Transformer-8-MultiHeadSelfAttention/dense_50/kernel:0 (768, 768)\n",
      "Transformer-8-MultiHeadSelfAttention/dense_50/bias:0 (768,)\n",
      "Transformer-8-MultiHeadSelfAttention/dense_51/kernel:0 (768, 768)\n",
      "Transformer-8-MultiHeadSelfAttention/dense_51/bias:0 (768,)\n",
      "Transformer-8-MultiHeadSelfAttention/dense_52/kernel:0 (768, 768)\n",
      "Transformer-8-MultiHeadSelfAttention/dense_52/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-8-MultiHeadSelfAttention-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-8-MultiHeadSelfAttention-Add\n",
      "-------------------------\n",
      "layer name Transformer-8-MultiHeadSelfAttention-Norm\n",
      "Transformer-8-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-8-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-8-FeedForward\n",
      "Transformer-8-FeedForward/dense_53/kernel:0 (768, 3072)\n",
      "Transformer-8-FeedForward/dense_53/bias:0 (3072,)\n",
      "Transformer-8-FeedForward/dense_54/kernel:0 (3072, 768)\n",
      "Transformer-8-FeedForward/dense_54/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-8-FeedForward-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-8-FeedForward-Add\n",
      "-------------------------\n",
      "layer name Transformer-8-FeedForward-Norm\n",
      "Transformer-8-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-8-FeedForward-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-9-MultiHeadSelfAttention\n",
      "Transformer-9-MultiHeadSelfAttention/dense_55/kernel:0 (768, 768)\n",
      "Transformer-9-MultiHeadSelfAttention/dense_55/bias:0 (768,)\n",
      "Transformer-9-MultiHeadSelfAttention/dense_56/kernel:0 (768, 768)\n",
      "Transformer-9-MultiHeadSelfAttention/dense_56/bias:0 (768,)\n",
      "Transformer-9-MultiHeadSelfAttention/dense_57/kernel:0 (768, 768)\n",
      "Transformer-9-MultiHeadSelfAttention/dense_57/bias:0 (768,)\n",
      "Transformer-9-MultiHeadSelfAttention/dense_58/kernel:0 (768, 768)\n",
      "Transformer-9-MultiHeadSelfAttention/dense_58/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-9-MultiHeadSelfAttention-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-9-MultiHeadSelfAttention-Add\n",
      "-------------------------\n",
      "layer name Transformer-9-MultiHeadSelfAttention-Norm\n",
      "Transformer-9-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-9-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-9-FeedForward\n",
      "Transformer-9-FeedForward/dense_59/kernel:0 (768, 3072)\n",
      "Transformer-9-FeedForward/dense_59/bias:0 (3072,)\n",
      "Transformer-9-FeedForward/dense_60/kernel:0 (3072, 768)\n",
      "Transformer-9-FeedForward/dense_60/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-9-FeedForward-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-9-FeedForward-Add\n",
      "-------------------------\n",
      "layer name Transformer-9-FeedForward-Norm\n",
      "Transformer-9-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-9-FeedForward-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-10-MultiHeadSelfAttention\n",
      "Transformer-10-MultiHeadSelfAttention/dense_61/kernel:0 (768, 768)\n",
      "Transformer-10-MultiHeadSelfAttention/dense_61/bias:0 (768,)\n",
      "Transformer-10-MultiHeadSelfAttention/dense_62/kernel:0 (768, 768)\n",
      "Transformer-10-MultiHeadSelfAttention/dense_62/bias:0 (768,)\n",
      "Transformer-10-MultiHeadSelfAttention/dense_63/kernel:0 (768, 768)\n",
      "Transformer-10-MultiHeadSelfAttention/dense_63/bias:0 (768,)\n",
      "Transformer-10-MultiHeadSelfAttention/dense_64/kernel:0 (768, 768)\n",
      "Transformer-10-MultiHeadSelfAttention/dense_64/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-10-MultiHeadSelfAttention-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-10-MultiHeadSelfAttention-Add\n",
      "-------------------------\n",
      "layer name Transformer-10-MultiHeadSelfAttention-Norm\n",
      "Transformer-10-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-10-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-10-FeedForward\n",
      "Transformer-10-FeedForward/dense_65/kernel:0 (768, 3072)\n",
      "Transformer-10-FeedForward/dense_65/bias:0 (3072,)\n",
      "Transformer-10-FeedForward/dense_66/kernel:0 (3072, 768)\n",
      "Transformer-10-FeedForward/dense_66/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-10-FeedForward-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-10-FeedForward-Add\n",
      "-------------------------\n",
      "layer name Transformer-10-FeedForward-Norm\n",
      "Transformer-10-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-10-FeedForward-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-11-MultiHeadSelfAttention\n",
      "Transformer-11-MultiHeadSelfAttention/dense_67/kernel:0 (768, 768)\n",
      "Transformer-11-MultiHeadSelfAttention/dense_67/bias:0 (768,)\n",
      "Transformer-11-MultiHeadSelfAttention/dense_68/kernel:0 (768, 768)\n",
      "Transformer-11-MultiHeadSelfAttention/dense_68/bias:0 (768,)\n",
      "Transformer-11-MultiHeadSelfAttention/dense_69/kernel:0 (768, 768)\n",
      "Transformer-11-MultiHeadSelfAttention/dense_69/bias:0 (768,)\n",
      "Transformer-11-MultiHeadSelfAttention/dense_70/kernel:0 (768, 768)\n",
      "Transformer-11-MultiHeadSelfAttention/dense_70/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-11-MultiHeadSelfAttention-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-11-MultiHeadSelfAttention-Add\n",
      "-------------------------\n",
      "layer name Transformer-11-MultiHeadSelfAttention-Norm\n",
      "Transformer-11-MultiHeadSelfAttention-Norm/beta:0 (768,)\n",
      "Transformer-11-MultiHeadSelfAttention-Norm/gamma:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-11-FeedForward\n",
      "Transformer-11-FeedForward/dense_71/kernel:0 (768, 3072)\n",
      "Transformer-11-FeedForward/dense_71/bias:0 (3072,)\n",
      "Transformer-11-FeedForward/dense_72/kernel:0 (3072, 768)\n",
      "Transformer-11-FeedForward/dense_72/bias:0 (768,)\n",
      "-------------------------\n",
      "layer name Transformer-11-FeedForward-Dropout\n",
      "-------------------------\n",
      "layer name Transformer-11-FeedForward-Add\n",
      "-------------------------\n",
      "layer name Transformer-11-FeedForward-Norm\n",
      "Transformer-11-FeedForward-Norm/beta:0 (768,)\n",
      "Transformer-11-FeedForward-Norm/gamma:0 (768,)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print('-------------------------')\n",
    "    print(f'layer name {layer.name}')\n",
    "    for weight in layer.weights:\n",
    "        print(weight.name, weight.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05685756 -0.01691563 -0.18896337 ...  0.24470313 -0.10127556\n",
      "   0.01406233]\n",
      " [-0.05613437 -0.10988988  0.35343567 ... -0.06752219 -0.22311936\n",
      "  -0.04273423]\n",
      " [-0.13579732 -0.18542694  0.0312222  ...  0.05397794 -0.02238331\n",
      "  -0.23441242]\n",
      " ...\n",
      " [-0.06893474 -0.28299838  0.2841855  ...  0.0912203  -0.13070367\n",
      "   0.13710706]\n",
      " [ 0.16877264  0.27536106  0.20753025 ... -0.06487323  0.0637659\n",
      "  -0.15097073]\n",
      " [-0.04221877 -0.18164001  0.02198964 ... -0.07176036 -0.04935543\n",
      "   0.19380973]]\n"
     ]
    }
   ],
   "source": [
    "print(lm_block.get_layer(index=0).get_weights()[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "lm_block_layer1 = lm_block.get_layer(index=0)\n",
    "bert_layer8 = model.get_layer(name='Transformer-0-MultiHeadSelfAttention')\n",
    "lm_block_layer1_weight = lm_block_layer1.get_weights()\n",
    "bert_layer8_weight = bert_layer8.get_weights()\n",
    "# 得到lm_block的第二层和bert的第八层(transformer的第一层)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(lm_block_layer1_weight)):\n",
    "    print(lm_block_layer1_weight[i].shape == bert_layer8_weight[i].shape)\n",
    "    # 维度完全相同"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "lm_block_layer1.set_weights(bert_layer8_weight)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.1491621e-01  6.5357164e-03  1.3464041e-02 ... -5.0526168e-02\n",
      "   1.7295334e-02  1.5026121e-02]\n",
      " [-9.4047729e-03 -2.2129135e-02  2.4954821e-03 ...  2.3847362e-02\n",
      "  -8.8263817e-02 -2.9466402e-02]\n",
      " [ 5.8782017e-03 -6.7798183e-03  3.5073500e-02 ...  1.0479877e-02\n",
      "  -5.6218460e-02 -1.5802451e-03]\n",
      " ...\n",
      " [ 1.3729039e-02  4.7092853e-05  1.1137443e-01 ...  5.6518074e-02\n",
      "  -4.2443074e-02  9.7083911e-02]\n",
      " [ 1.5892318e-02  4.5497168e-02 -3.1567805e-02 ...  4.1080634e-03\n",
      "   3.8016669e-02 -3.4227695e-02]\n",
      " [-8.1710182e-02  1.3203139e-02 -1.4776111e-02 ...  7.1023442e-02\n",
      "  -1.5387528e-02  2.2910309e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(lm_block.get_layer(index=0).get_weights()[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1.], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(lm_block.get_layer(index=1).get_weights())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "lm_block_layer2 = lm_block.get_layer(index=1)\n",
    "bert_layer11 = model.get_layer(index=11)\n",
    "lm_block_layer2_weight = lm_block_layer2.get_weights()\n",
    "bert_layer11_weight = bert_layer11.get_weights()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(lm_block_layer2_weight)):\n",
    "    print(lm_block_layer2_weight[i].shape == bert_layer11_weight[i].shape)\n",
    "    # 维度完全相同"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "lm_block_layer2.set_weights(bert_layer11_weight)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 1.16601745e-02,  5.40437289e-02, -9.16207060e-02,  1.05281323e-01,\n",
      "        3.48078430e-01, -1.68094128e-01,  7.95071498e-02, -8.63028504e-03,\n",
      "       -6.27969205e-02, -2.64445066e-01,  2.60950685e-01, -2.48833984e-01,\n",
      "       -1.17190272e-01, -1.13512546e-01,  1.55974582e-01, -2.52695531e-02,\n",
      "        1.71177953e-01, -5.15815839e-02,  7.09559023e-02, -1.18340828e-01,\n",
      "        2.12990418e-02,  1.26445740e-01,  1.75389534e-04, -1.34746641e-01,\n",
      "       -1.15555532e-01, -2.60010883e-02,  7.14812949e-02, -7.86032081e-02,\n",
      "       -1.22626677e-01,  2.22578004e-01,  5.76628707e-02, -1.35067642e-01,\n",
      "       -4.47699465e-02, -2.01051325e-01,  5.94254211e-02,  1.82048753e-01,\n",
      "        1.29285902e-02, -1.16891749e-02, -2.27592885e-02,  2.86168993e-01,\n",
      "        2.68879887e-02, -1.13643475e-01,  3.20790112e-02, -9.24040452e-02,\n",
      "       -8.81051831e-03,  1.03911594e-01,  2.40146920e-01,  2.45545626e-01,\n",
      "       -3.97668779e-02, -1.32390097e-01,  1.58876464e-01,  1.41543996e+00,\n",
      "        3.39075364e-02, -6.71935454e-02,  1.95914116e-02, -1.19478153e-02,\n",
      "       -6.56981543e-02, -3.30787525e-02,  1.83109462e-01, -1.09650500e-01,\n",
      "        8.77996162e-02,  1.93747088e-01, -4.84883301e-02,  1.54646248e-01,\n",
      "        6.45855814e-02, -1.43346086e-01, -1.31121859e-01,  1.94503799e-01,\n",
      "       -3.15578724e-03, -6.75573945e-03, -1.79131143e-02, -4.66299281e-02,\n",
      "       -1.18533149e-01, -1.88724622e-02,  4.39491868e-03, -4.98885885e-02,\n",
      "       -6.18789857e-03,  2.56574973e-02, -1.23420157e-01, -1.21744983e-01,\n",
      "        1.99776694e-01,  1.30321188e-02, -9.59855244e-02,  1.50181487e-01,\n",
      "       -4.07571457e-02, -1.40431285e-01, -3.34056392e-02,  1.46248853e+00,\n",
      "       -1.12871356e-01,  4.83622178e-02, -1.54974565e-01, -1.66904867e-01,\n",
      "        1.18957847e-01,  3.49389054e-02,  6.26970232e-02,  5.71195893e-02,\n",
      "       -2.04101242e-02, -1.08180389e-01,  9.32455063e-02, -1.89845249e-01,\n",
      "        4.08390343e-01, -1.44534469e-01, -2.76573021e-02,  5.64635023e-02,\n",
      "        1.51166424e-01,  2.46285424e-01,  2.06312194e-01,  8.33104551e-03,\n",
      "        2.28545412e-01, -8.12742338e-02, -7.91509673e-02,  7.98407346e-02,\n",
      "       -9.96465459e-02,  2.11478453e-02,  8.97396579e-02, -2.48772278e-01,\n",
      "       -9.44110155e-02, -9.69332531e-02,  1.00152753e-01, -2.47843545e-02,\n",
      "        1.14579976e-01,  1.52224287e-01,  9.84936357e-02,  1.26029298e-01,\n",
      "       -9.87350866e-02,  2.29858365e-02,  1.49896890e-01, -6.76782206e-02,\n",
      "        5.97343072e-02, -1.80849373e-01,  8.83962661e-02, -1.08865634e-01,\n",
      "       -6.22474849e-02, -1.57646406e-02, -5.99827804e-02, -1.67670414e-01,\n",
      "        3.75449588e-03,  6.58584088e-02, -7.56445229e-02,  1.98811218e-01,\n",
      "        2.94602308e-02, -8.03459883e-02, -5.00970595e-02, -9.60948039e-03,\n",
      "        8.14753696e-02,  4.98484671e-02,  1.41294628e-01,  1.20188430e-01,\n",
      "        1.40287951e-01,  9.14383978e-02,  7.82202631e-02, -9.67593212e-03,\n",
      "        2.35546663e-01, -3.53653222e-01,  5.56358546e-02,  1.06689513e-01,\n",
      "        5.27784787e-02,  4.48672734e-02,  5.59033938e-02,  3.49275372e-03,\n",
      "       -1.87480554e-01, -5.81463724e-02, -5.46531081e-02,  2.63387918e-01,\n",
      "       -1.29154563e-01, -4.56994437e-02, -2.39934325e-02,  1.86516926e-01,\n",
      "       -4.13046964e-02, -1.15621634e-01,  9.41777155e-02,  9.43355411e-02,\n",
      "       -2.06652861e-02, -1.68292642e-01,  8.43255818e-02,  1.08218595e-01,\n",
      "        1.05052302e-02,  3.78864072e-02, -4.95727867e-01, -1.39135092e-01,\n",
      "       -3.20214927e-02,  6.99431170e-03, -1.25762890e-03,  2.52782971e-01,\n",
      "        6.06331155e-02, -1.66934002e-02, -1.72284115e-02,  1.69328928e-01,\n",
      "       -4.91370231e-01,  4.84862886e-02, -2.75935292e-01,  1.46752391e-02,\n",
      "       -5.08124493e-02, -1.82505846e-01, -8.70010182e-02, -7.46128634e-02,\n",
      "       -9.46817458e-01,  4.78683934e-02, -5.09049743e-02,  1.50807485e-01,\n",
      "       -1.47690788e-01, -2.76172459e-01,  6.80856686e-03, -2.91573972e-01,\n",
      "       -8.81694481e-02,  2.04183072e-01,  3.49038914e-02, -4.31464147e-03,\n",
      "       -6.97182864e-02, -4.35413837e-01,  5.68097681e-02, -2.24571228e-02,\n",
      "       -2.52509981e-01, -1.77422836e-01, -1.94738746e-01, -7.28493184e-02,\n",
      "       -1.88076079e-01,  1.21804625e-02, -3.96561027e-02, -4.56399359e-02,\n",
      "        1.12242252e-01,  6.52998239e-02, -1.80977639e-02, -1.24285735e-01,\n",
      "        5.49894236e-02,  1.64526388e-01,  1.02562448e-02, -9.05669183e-02,\n",
      "       -1.16835097e-02,  2.72029161e-01,  5.69901988e-02, -3.00458819e-01,\n",
      "       -1.84047773e-01, -1.44317016e-01, -1.42408311e-01,  2.80638020e-02,\n",
      "        3.73910591e-02, -9.88574773e-02, -4.31177877e-02,  6.94442093e-02,\n",
      "        5.99282514e-03, -2.96218127e-01,  1.66282415e-01, -2.93290883e-01,\n",
      "       -2.24175215e-01,  2.92887446e-02, -1.70158729e-01, -7.37767741e-02,\n",
      "       -3.68290469e-02, -3.76198255e-03,  4.70160581e-02,  1.05188169e-01,\n",
      "        1.90238982e-01, -6.64078146e-02,  8.95822123e-02, -1.27502501e-01,\n",
      "        1.43037125e-01,  4.79880907e-03, -1.34116635e-01,  1.03842087e-01,\n",
      "        4.59219605e-01, -3.31022367e-02,  7.20479572e-03, -3.84300679e-01,\n",
      "       -5.78377247e-02, -1.44054919e-01, -1.27893582e-01, -1.57083079e-01,\n",
      "        5.03257886e-02, -6.25083372e-02,  8.48371834e-02,  4.22324203e-02,\n",
      "       -3.10207251e-02, -4.52160165e-02,  1.66353360e-01, -1.80889830e-01,\n",
      "       -1.08188488e-01, -1.92643642e-01,  3.71517129e-02, -2.66198099e-01,\n",
      "       -4.50306162e-02,  3.29191051e-02, -1.10227101e-01, -1.00857317e-01,\n",
      "        2.31675878e-02,  2.36420855e-01,  4.04809773e-01, -5.74335977e-02,\n",
      "        3.41016352e-02,  5.46556339e-02,  5.75197078e-02,  1.54175624e-01,\n",
      "       -5.62891029e-02,  1.62727177e-01,  1.31982410e+00, -4.00642790e-02,\n",
      "        3.05717319e-01, -1.60015419e-01,  1.71734840e-01, -2.14885414e-01,\n",
      "        5.88256679e-02,  1.46388441e-01, -1.82446554e-01, -1.59890190e-01,\n",
      "       -8.95978212e-02, -1.33097759e-02, -3.96808535e-02, -6.62692711e-02,\n",
      "       -4.89365458e-02,  1.09992899e-01,  4.42343615e-02, -1.98902428e-01,\n",
      "       -6.07860554e-03,  1.19004525e-01,  2.35550608e-02, -6.27055392e-02,\n",
      "        1.18214533e-01,  9.28324386e-02,  6.02064701e-03,  1.91926342e-02,\n",
      "       -7.54593685e-02, -1.79111183e-01, -9.41486191e-03,  1.12217134e-02,\n",
      "        9.12665054e-02, -1.18623406e-01,  2.80031450e-02, -5.87907562e-04,\n",
      "        4.38600741e-02,  7.68647119e-02, -4.00832109e-02,  3.97882648e-02,\n",
      "       -5.11628389e-02, -3.55508447e-01, -5.33001050e-02, -4.75006290e-02,\n",
      "       -3.97997983e-02, -3.86211187e-01,  2.81849593e-01,  6.43573701e-02,\n",
      "       -6.53544515e-02,  2.17816472e-01, -1.59650087e-01, -6.80238456e-02,\n",
      "        8.15159827e-02, -5.69958873e-02,  8.87070447e-02, -1.17199890e-01,\n",
      "       -1.39789760e-01, -8.01332295e-03,  4.44311686e-02, -1.58577189e-01,\n",
      "        1.46975383e-01,  2.06136838e-01,  9.85755678e-03, -3.84434074e-01,\n",
      "       -1.21661134e-01,  1.49370238e-01, -3.61958668e-02, -5.99705614e-02,\n",
      "       -7.05503449e-02,  8.44841003e-02,  5.78458309e-02,  1.69521526e-01,\n",
      "       -5.10634363e-01, -1.54446155e-01,  1.05450906e-01, -2.45926566e-02,\n",
      "        6.65800348e-02, -9.61029753e-02, -1.08635269e-01, -1.26639739e-01,\n",
      "       -4.04359512e-02, -7.90308788e-02, -1.34867311e-01, -1.43744379e-01,\n",
      "       -7.61212260e-02, -2.38848820e-01, -1.16709597e-01, -1.87723730e-02,\n",
      "        8.77702832e-02, -1.05399750e-01,  3.51512618e-03, -4.66708653e-02,\n",
      "        4.61780168e-02,  1.97888333e-02,  2.57867831e-03,  2.22004447e-02,\n",
      "       -2.37785488e-01,  1.88416988e-01,  9.66105983e-03,  5.59960827e-02,\n",
      "        1.89600617e-01, -3.58702131e-02,  1.45897970e-01, -1.84984103e-01,\n",
      "       -1.47286102e-01, -1.20882250e-01, -6.43306673e-02, -3.53749171e-02,\n",
      "       -5.64280599e-02, -7.90552348e-02,  7.37828016e-02,  1.87022880e-01,\n",
      "        1.79367945e-01,  1.54616266e-01,  5.80487773e-02, -7.82992691e-02,\n",
      "       -4.99491207e-02,  8.35647658e-02, -2.07198739e-01, -2.00035065e-01,\n",
      "       -1.22656263e-01,  1.86496362e-01, -2.45274100e-02, -9.08095688e-02,\n",
      "        3.26538160e-02,  2.36515343e-01, -7.56635368e-01, -6.07909262e-02,\n",
      "       -4.41445969e-02,  1.61862314e-01, -2.68980354e-01, -1.10568039e-01,\n",
      "       -7.33072832e-02, -1.05274715e-01,  1.82593182e-01,  1.54959336e-01,\n",
      "        2.91759968e-02, -4.85024601e-02, -3.23139392e-02,  3.10314950e-02,\n",
      "        6.25378862e-02,  1.86879542e-02, -2.15151623e-01,  3.98273677e-01,\n",
      "        5.31288385e-02, -4.98582758e-02, -7.30822561e-03,  1.11903407e-01,\n",
      "       -5.39419055e-03, -3.87980118e-02, -5.24027795e-02, -1.15406126e-01,\n",
      "        5.33949211e-02, -7.25599155e-02, -1.81317136e-01, -1.66578040e-01,\n",
      "        2.12073252e-02, -1.59441739e-01, -3.58920209e-02, -2.97448426e-01,\n",
      "       -1.29124865e-01,  2.34219469e-02, -9.05293692e-03, -2.29707696e-02,\n",
      "       -7.12876841e-02,  2.29359251e-02,  1.44651428e-01, -1.22703508e-01,\n",
      "       -3.41942310e-02, -3.71771269e-02, -1.62899435e-01, -3.74221742e-01,\n",
      "        1.63029149e-01, -1.18140727e-01,  2.28389259e-02, -7.20071197e-02,\n",
      "        5.86545877e-02, -3.77522945e-01, -3.20968777e-02,  7.52110854e-02,\n",
      "       -1.06470898e-01,  2.68774554e-02, -2.27189571e-01, -2.34779790e-01,\n",
      "       -4.61201556e-02,  8.41126367e-02, -2.12169476e-02,  5.72440121e-03,\n",
      "       -1.04196919e-02,  7.20626786e-02,  3.19000818e-02, -2.50867963e-01,\n",
      "       -2.94627875e-01,  1.20095178e-01, -2.24563196e-01, -6.10970780e-02,\n",
      "       -1.16946131e-01, -1.83158845e-01,  4.21013013e-02, -1.57277986e-01,\n",
      "       -1.18755110e-01, -7.78610557e-02, -2.97554135e-01, -8.86073783e-02,\n",
      "       -7.01272413e-02, -1.84425831e-01,  1.03521213e-01, -3.22348237e-01,\n",
      "       -2.92636920e-02, -7.94065446e-02,  1.57285392e-01, -1.69653296e-01,\n",
      "        4.04947773e-02, -1.25210881e-01,  9.25727263e-02, -5.15491784e-01,\n",
      "       -4.10069525e-02, -1.64802179e-01,  1.08515337e-01,  9.77054611e-02,\n",
      "       -3.97894047e-02,  9.00733471e-02, -1.34090468e-01,  4.98283282e-02,\n",
      "        1.37047201e-01, -1.88303038e-01,  5.49758412e-02, -4.94739553e-03,\n",
      "        5.79750761e-02,  2.96217985e-02, -1.74583076e-03,  5.35721108e-02,\n",
      "       -4.16570678e-02,  3.26032974e-02,  2.27964409e-02,  1.16726741e-01,\n",
      "       -7.89375380e-02,  4.21279185e-02, -9.37493071e-02, -4.42917459e-02,\n",
      "       -9.52662379e-02,  2.39908844e-01,  9.05941147e-03, -1.71364442e-01,\n",
      "       -1.94135785e-01,  5.29534340e-01,  1.09479632e-02,  1.58317536e-01,\n",
      "       -1.46973254e-02, -5.22134192e-02,  4.96720858e-02,  1.92322865e-01,\n",
      "       -1.59429878e-01, -7.47699440e-02, -4.03786488e-02, -8.55626538e-03,\n",
      "       -7.61137754e-02,  5.60862310e-02, -8.51278473e-03, -3.49354118e-01,\n",
      "        8.18623900e-02, -4.19981927e-02,  6.83378801e-02,  3.56160760e-01,\n",
      "       -2.78850824e-01, -7.87936226e-02,  2.22026452e-01,  2.11645514e-02,\n",
      "       -1.41225502e-01, -2.28932071e-02, -5.13708219e-02, -9.10613090e-02,\n",
      "       -4.49096918e-01,  1.14195168e-01, -5.78909554e-02, -2.69133947e-03,\n",
      "       -3.20981536e-03, -5.45488819e-02,  6.96854740e-02,  9.09270160e-03,\n",
      "        1.81140706e-01,  3.11055124e-01,  3.73128839e-02,  3.75284404e-02,\n",
      "       -8.43271166e-02, -1.94227528e-02,  4.74261343e-02,  1.63973551e-02,\n",
      "       -1.11390240e-01, -1.03258707e-01,  1.02569550e-01,  8.20974335e-02,\n",
      "        1.26102582e-01, -1.27697974e-01,  5.30013204e-01, -1.45584390e-01,\n",
      "        6.62780106e-02,  8.31677392e-02,  8.82406011e-02,  9.20442492e-02,\n",
      "        3.98601182e-02, -1.78374290e-01,  6.43705502e-02,  4.68840525e-02,\n",
      "        1.57665670e-01,  1.54548913e-01,  8.47817436e-02,  4.01721336e-02,\n",
      "        2.34738290e-01, -9.11495090e-02,  6.34457245e-02,  1.43436670e-01,\n",
      "        1.77494615e-01,  1.97381765e-01, -1.37220934e-01,  4.49299179e-02,\n",
      "       -4.09128010e-01, -6.76315203e-02, -3.42977094e-03,  9.84494090e-02,\n",
      "       -8.93563777e-02, -1.77147277e-02,  6.03637770e-02, -6.43892884e-02,\n",
      "        6.51191846e-02,  1.68314144e-01, -5.68215037e-03,  1.30484492e-01,\n",
      "        1.94348067e-01, -2.10738569e-01, -1.33081287e-01, -2.38845140e-01,\n",
      "       -8.20978731e-02,  1.57852501e-01,  5.53742349e-01, -8.62374082e-02,\n",
      "        1.22005217e-01, -1.75259635e-01, -8.46777633e-02, -2.89564412e-02,\n",
      "        7.91603848e-02, -1.94552150e-02,  1.60139337e-01,  5.20656705e-02,\n",
      "       -4.03080955e-02,  1.01162247e-01, -1.34461746e-01,  9.96705666e-02,\n",
      "       -2.03439686e-02,  1.97665051e-01, -6.08823337e-02,  1.76813193e-02,\n",
      "       -1.39605284e-01, -1.04636297e-01,  3.03230174e-02,  1.11016743e-02,\n",
      "        4.65593860e-02,  1.45068914e-01, -6.09812886e-02, -7.18360115e-03,\n",
      "       -1.10168621e-01,  4.27723117e-02,  1.67675257e-01, -8.40498358e-02,\n",
      "       -8.19842592e-02,  2.73649275e-01,  5.23488522e-02, -6.54463544e-02,\n",
      "       -1.04996824e+00,  4.23150547e-02, -4.32458445e-02, -1.52939007e-01,\n",
      "        1.78824201e-01, -6.76961839e-02, -5.20441309e-02,  1.07100584e-01,\n",
      "        7.64356107e-02, -1.24425448e-01, -7.50221387e-02, -1.87623531e-01,\n",
      "        1.82412088e-01, -2.27947272e-02, -4.63292897e-02, -8.65216330e-02,\n",
      "        2.06781566e-01, -9.92595963e-03,  1.77772939e-01, -1.57979816e-01,\n",
      "       -2.43946180e-01, -1.30095318e-01,  4.46688011e-03,  1.03983805e-01,\n",
      "        3.84573162e-01, -3.22654136e-02, -2.07112402e-01, -8.72794017e-02,\n",
      "       -9.81824026e-02,  2.02780724e-01, -1.44680813e-01, -2.44577471e-02,\n",
      "       -1.84898421e-01, -6.55188486e-02,  4.58436534e-02, -6.39368445e-02,\n",
      "        3.13707367e-02, -4.05125692e-02, -1.64078131e-01,  3.88342366e-02,\n",
      "       -9.42827947e-03, -3.34728998e-03, -1.05477192e-01, -1.19836204e-01,\n",
      "       -2.55255606e-02, -2.42898725e-02, -3.21597494e-02, -2.45893616e-02,\n",
      "        5.16705262e-03,  1.04943905e-02,  3.91918659e-01,  6.34480044e-02,\n",
      "       -2.37141758e-01,  3.35044377e-02, -1.97528955e-02, -9.33660939e-03,\n",
      "        4.23362739e-02, -1.30796491e-03, -1.57576233e-01, -4.19196129e-01,\n",
      "        2.73990601e-01,  4.95637394e-02,  1.02310814e-01, -3.65090184e-02,\n",
      "        6.01740647e-03,  2.69607842e-01,  2.35121295e-01, -1.75552279e-01,\n",
      "       -7.23980367e-02,  9.29077119e-02,  9.30743739e-02, -1.79747447e-01,\n",
      "        5.44900335e-02,  1.42220452e-01, -1.07670061e-01,  1.17835268e-01,\n",
      "       -8.91078338e-02,  5.65979540e-01, -2.64453173e-01,  2.26029098e-01,\n",
      "        6.97753206e-02,  4.04296182e-02,  5.81982583e-02,  9.58672762e-02,\n",
      "        1.09399796e-01, -1.86591685e-01,  6.45414516e-02,  2.69681305e-01,\n",
      "        1.78947989e-02,  3.27344835e-02, -7.88120851e-02,  1.00623265e-01,\n",
      "        3.26020569e-02, -1.64472908e-01,  1.41284645e-01, -1.03394166e-01,\n",
      "       -4.54750061e+00,  2.50396971e-02,  3.99254933e-02,  4.58940752e-02,\n",
      "        3.73444557e-02,  2.83888057e-02, -2.30628088e-01, -9.97446179e-02,\n",
      "        2.68968612e-01, -1.60915941e-01,  1.18168229e-02, -4.90448661e-02],\n",
      "      dtype=float32), array([0.8443915 , 0.8265491 , 0.84417963, 0.82296133, 0.802638  ,\n",
      "       0.8244341 , 0.822517  , 0.81708664, 0.79406613, 0.80531543,\n",
      "       0.8440844 , 0.8130767 , 0.75517887, 0.80912775, 0.8976842 ,\n",
      "       0.8253196 , 0.83015144, 0.83508265, 0.82252836, 0.8163895 ,\n",
      "       0.812086  , 0.7945965 , 0.83219093, 0.8398559 , 0.81743324,\n",
      "       0.8375152 , 0.81187415, 0.8240624 , 0.85153365, 0.8128465 ,\n",
      "       0.7896793 , 0.8623569 , 0.8353036 , 0.8168127 , 0.827001  ,\n",
      "       0.81869274, 0.8483519 , 0.793305  , 0.8200991 , 0.86305815,\n",
      "       0.8150355 , 0.83855855, 0.84739214, 0.8775763 , 0.7906334 ,\n",
      "       0.83060706, 0.82510847, 0.8480135 , 0.81631225, 0.81967455,\n",
      "       0.81194985, 1.9895811 , 0.7996255 , 0.8368675 , 0.8244771 ,\n",
      "       0.8225545 , 0.80660796, 0.8289318 , 0.829516  , 0.85737234,\n",
      "       0.8554463 , 0.8368381 , 0.82378423, 0.8025202 , 0.8516307 ,\n",
      "       0.8076345 , 0.835027  , 0.8112135 , 0.8206556 , 0.795671  ,\n",
      "       0.8192087 , 0.8104387 , 0.8413716 , 0.8253983 , 0.8389487 ,\n",
      "       0.7992225 , 0.8245917 , 0.81983507, 0.7954911 , 0.83587873,\n",
      "       0.8659885 , 0.8061383 , 0.80429083, 0.8257495 , 0.8151583 ,\n",
      "       0.86741483, 0.82622737, 0.8684442 , 0.8327409 , 0.83161557,\n",
      "       0.85818446, 0.8242564 , 0.8148848 , 0.83646333, 0.8194153 ,\n",
      "       1.4697146 , 0.7985561 , 0.83220917, 0.8352517 , 0.86667603,\n",
      "       0.711854  , 0.8341243 , 0.8137809 , 0.85242915, 0.8575811 ,\n",
      "       0.8796911 , 0.84045994, 0.8220677 , 0.8172169 , 0.8149116 ,\n",
      "       0.8584699 , 0.8281271 , 0.82888454, 0.8185973 , 0.82334846,\n",
      "       0.83044165, 0.86385375, 0.81684417, 0.8382539 , 0.8246935 ,\n",
      "       0.821769  , 0.850569  , 0.8244042 , 0.8250581 , 0.84359866,\n",
      "       0.8178593 , 0.8225714 , 0.84369993, 0.8391536 , 0.8196355 ,\n",
      "       0.825653  , 0.83607423, 0.8230191 , 0.84275526, 0.85738486,\n",
      "       0.84309214, 0.82502264, 0.81721455, 0.8327684 , 0.8259126 ,\n",
      "       0.8663401 , 0.86004215, 0.82680535, 0.82106686, 0.8298357 ,\n",
      "       0.8039613 , 0.82296634, 0.83194506, 0.8402429 , 0.8115296 ,\n",
      "       0.8343755 , 0.8052088 , 0.83477217, 0.801248  , 0.8185731 ,\n",
      "       0.8270342 , 0.8240613 , 0.86713326, 0.812434  , 0.8365366 ,\n",
      "       0.8187297 , 0.82446116, 0.84063953, 0.84017754, 0.83871824,\n",
      "       0.8427206 , 0.8540887 , 0.82768446, 0.81474876, 0.8087321 ,\n",
      "       0.7553186 , 0.83525205, 0.8253272 , 0.8562081 , 0.80950385,\n",
      "       0.8386497 , 0.8403148 , 0.84278655, 0.7505346 , 0.84995043,\n",
      "       0.8357638 , 0.8461417 , 0.8041159 , 0.84636366, 0.7738657 ,\n",
      "       0.8216182 , 0.80869824, 0.82291263, 0.67449653, 0.8673766 ,\n",
      "       0.82137483, 0.8058299 , 1.0001822 , 0.82971686, 0.79710466,\n",
      "       0.4682183 , 0.8405261 , 0.84466964, 0.798062  , 0.8180473 ,\n",
      "       0.7759385 , 0.8120362 , 0.82989186, 0.8250641 , 0.82412046,\n",
      "       0.83191144, 0.8262377 , 0.82249254, 0.82021236, 0.84830326,\n",
      "       0.85175365, 0.8212881 , 0.84468174, 0.8243675 , 0.82888937,\n",
      "       0.8343579 , 0.83788747, 0.8078394 , 0.8003361 , 0.8348655 ,\n",
      "       0.8124166 , 0.81958926, 0.8451204 , 0.8478221 , 0.8293605 ,\n",
      "       0.8153517 , 0.8401544 , 0.81087255, 0.8123784 , 0.8666223 ,\n",
      "       0.8312191 , 0.84772295, 0.8376954 , 0.796759  , 0.8126131 ,\n",
      "       0.8403276 , 0.8380177 , 0.82235193, 0.80405277, 0.8216058 ,\n",
      "       0.83304286, 0.824444  , 0.83343214, 0.5297027 , 0.8324206 ,\n",
      "       0.82305217, 0.81887627, 0.8598297 , 0.83290154, 0.82164323,\n",
      "       0.8390595 , 0.83521223, 0.7751746 , 0.82894015, 0.8143047 ,\n",
      "       0.82996225, 0.84722173, 0.82065004, 0.830788  , 0.83301693,\n",
      "       0.98920643, 0.85005766, 0.8485415 , 0.8689523 , 0.89692223,\n",
      "       0.6624694 , 0.82754856, 0.83097774, 0.82746804, 0.8291495 ,\n",
      "       0.824155  , 0.8133331 , 0.83972514, 0.8041298 , 0.83487916,\n",
      "       0.82560843, 0.8418648 , 0.8449586 , 0.8237696 , 0.82702935,\n",
      "       0.8290798 , 0.83553344, 0.8245679 , 0.82553446, 0.81392926,\n",
      "       0.8295556 , 0.8201404 , 0.82440877, 0.8092748 , 0.8480808 ,\n",
      "       0.85698175, 0.8419817 , 0.5971506 , 0.83833617, 1.1704612 ,\n",
      "       0.8275618 , 0.82324994, 0.8267836 , 0.844404  , 0.8186376 ,\n",
      "       0.86407095, 0.8353855 , 0.8377824 , 0.8548622 , 0.785824  ,\n",
      "       0.8288788 , 0.8029923 , 0.8006777 , 0.831015  , 0.80980104,\n",
      "       0.81612974, 0.80588126, 0.8390993 , 0.6947209 , 0.8061944 ,\n",
      "       0.81576747, 0.82901794, 0.84919995, 0.8248232 , 0.8339463 ,\n",
      "       0.83696234, 0.81304187, 0.8404159 , 0.8415165 , 0.8505924 ,\n",
      "       0.8433256 , 0.8417714 , 0.84642625, 0.85174245, 0.82649183,\n",
      "       0.8355513 , 0.8032113 , 0.83225983, 0.8724574 , 0.7964211 ,\n",
      "       0.8181655 , 0.8298674 , 0.8491736 , 0.8279671 , 0.78685045,\n",
      "       0.8295076 , 0.8590441 , 0.8641105 , 0.83434874, 0.8084995 ,\n",
      "       0.7985281 , 0.8325703 , 0.8327716 , 0.8208225 , 0.8466998 ,\n",
      "       0.8365085 , 0.82838136, 0.8058436 , 0.8176429 , 0.8310483 ,\n",
      "       0.8314128 , 0.71771884, 0.82477003, 0.82261676, 0.8212346 ,\n",
      "       0.84157896, 0.80911696, 0.8081255 , 0.81421953, 0.49077144,\n",
      "       0.82802415, 0.8072223 , 0.8209228 , 0.5354166 , 0.79567814,\n",
      "       0.8210189 , 0.8179971 , 0.7968531 , 0.83868057, 0.8156166 ,\n",
      "       0.821425  , 0.82520634, 0.82322425, 0.84890664, 0.81253654,\n",
      "       0.82736367, 0.84373534, 0.8003376 , 0.9068165 , 0.8074391 ,\n",
      "       0.79499054, 0.8322439 , 0.8278177 , 0.7715624 , 0.81532997,\n",
      "       0.81513906, 0.8169889 , 0.8253977 , 0.8397304 , 0.8504839 ,\n",
      "       0.6189807 , 0.81627834, 0.8085071 , 0.8123087 , 0.8390439 ,\n",
      "       0.8290721 , 0.8108857 , 0.8434456 , 0.7995204 , 0.84292394,\n",
      "       0.8524339 , 0.8314553 , 0.8225309 , 0.8091309 , 0.849462  ,\n",
      "       0.83802325, 0.83850855, 0.72031265, 0.8283664 , 0.80308014,\n",
      "       0.8436912 , 0.84991175, 0.8489627 , 0.80364895, 0.843698  ,\n",
      "       0.8282253 , 0.8007495 , 0.8486623 , 0.82462233, 0.7877802 ,\n",
      "       0.82629454, 0.86329573, 0.83292156, 0.8049669 , 0.8197597 ,\n",
      "       0.81735516, 0.84561324, 0.8320457 , 0.8267416 , 0.8667026 ,\n",
      "       0.8543905 , 0.8440307 , 0.8291737 , 0.81630516, 0.83829033,\n",
      "       0.8087233 , 0.8313233 , 0.83745843, 0.83863217, 0.8467209 ,\n",
      "       0.7807914 , 0.85217035, 0.8394858 , 0.80625296, 0.82589245,\n",
      "       0.83833796, 0.85384345, 0.8593434 , 0.8206374 , 0.81759506,\n",
      "       0.8319887 , 0.8288928 , 0.8201427 , 0.84983975, 0.7983605 ,\n",
      "       0.8307989 , 0.82928747, 0.86944646, 0.81376415, 0.8393919 ,\n",
      "       0.82363695, 0.8180745 , 0.84016186, 0.82518727, 0.8466385 ,\n",
      "       0.8154482 , 0.85091746, 0.816723  , 0.8215283 , 0.86466706,\n",
      "       0.8206611 , 0.83501107, 0.82963514, 0.87637794, 0.8851484 ,\n",
      "       0.8288905 , 0.82390654, 0.82216054, 0.8519216 , 0.9231035 ,\n",
      "       0.8206169 , 0.8285712 , 0.8409737 , 0.82056963, 0.83533704,\n",
      "       0.84569126, 0.8280285 , 0.8486826 , 0.830377  , 0.8347429 ,\n",
      "       0.85441554, 0.84892285, 0.8097299 , 0.83598566, 0.81844974,\n",
      "       0.80410963, 0.8356084 , 0.8447455 , 0.84063786, 0.83279264,\n",
      "       0.83689815, 0.80818164, 0.84843516, 0.8362829 , 0.84396327,\n",
      "       0.85986346, 0.82232183, 0.8373039 , 0.90812576, 0.8237529 ,\n",
      "       0.84340227, 0.82937384, 0.83630145, 0.8194128 , 0.82239515,\n",
      "       0.8189789 , 0.81801826, 0.8471484 , 0.82678753, 0.8461439 ,\n",
      "       0.8261014 , 0.8235681 , 0.7073529 , 0.7827926 , 0.8610244 ,\n",
      "       0.8424528 , 0.7981768 , 0.8359417 , 0.82984555, 0.8165627 ,\n",
      "       0.84850264, 0.8322464 , 0.848562  , 0.8103954 , 0.50415623,\n",
      "       0.8326934 , 0.87774855, 0.8212862 , 0.84386474, 0.79404926,\n",
      "       0.8869771 , 0.82177275, 0.812592  , 0.83187646, 0.82793885,\n",
      "       0.82752097, 0.8284178 , 0.8379286 , 0.8213772 , 0.84326965,\n",
      "       0.8182245 , 0.83816725, 0.8497318 , 0.83784175, 0.83097357,\n",
      "       0.83922756, 0.8161585 , 0.8486697 , 0.8246753 , 0.87170774,\n",
      "       0.82592916, 0.8301882 , 0.82329786, 0.7821648 , 0.8136805 ,\n",
      "       0.6919163 , 0.79214513, 0.8347127 , 0.84952325, 0.8208401 ,\n",
      "       0.8314488 , 0.8397816 , 0.83888686, 0.8116979 , 0.8512066 ,\n",
      "       0.8413447 , 0.83344626, 0.8201034 , 0.842864  , 0.79913175,\n",
      "       0.82935685, 0.8544199 , 0.8232009 , 0.8060495 , 0.64356506,\n",
      "       0.8303885 , 0.8347634 , 0.83533525, 0.8175217 , 0.83092546,\n",
      "       0.83536124, 0.8258561 , 0.8200463 , 0.8353163 , 0.8296378 ,\n",
      "       0.8079582 , 0.82812   , 0.83458966, 0.8277547 , 0.82191354,\n",
      "       0.78835857, 0.8201339 , 0.8236687 , 0.84695363, 0.83818686,\n",
      "       0.8059398 , 0.85810715, 0.80387276, 0.8319458 , 0.8354404 ,\n",
      "       0.84590256, 0.42255118, 0.84300387, 0.83137953, 0.83639926,\n",
      "       0.8500475 , 0.84801024, 0.84067   , 0.84998065, 0.8165157 ,\n",
      "       0.84902716, 1.1753833 , 0.8307429 , 0.82008994, 0.8475566 ,\n",
      "       0.83440423, 0.8227582 , 0.85525364, 0.82836854, 0.73242277,\n",
      "       0.8025673 , 0.8234189 , 0.5440698 , 0.8446746 , 0.8079186 ,\n",
      "       0.8322155 , 0.83536446, 0.8419764 , 0.83675474, 0.78789496,\n",
      "       0.82578874, 0.8178121 , 0.54138964, 0.83797026, 0.8436698 ,\n",
      "       0.8068121 , 0.81872636, 0.7914814 , 0.8093368 , 0.84256417,\n",
      "       0.81735903, 0.8335526 , 0.82441926, 0.81060696, 0.8069523 ,\n",
      "       0.5308139 , 0.80012405, 0.84987044, 0.84144866, 0.83252734,\n",
      "       0.8385865 , 0.82694185, 0.8268816 , 0.83470553, 0.83446485,\n",
      "       0.8359074 , 0.8261012 , 0.8480403 , 0.8266007 , 0.8220515 ,\n",
      "       0.8325728 , 0.85318077, 0.8267185 , 0.79995817, 0.81620365,\n",
      "       0.8446045 , 0.81922936, 0.84797513, 0.8447097 , 0.8724558 ,\n",
      "       0.83731556, 0.8487811 , 0.6814287 , 0.688951  , 0.52834374,\n",
      "       0.8386677 , 0.83130187, 0.8295328 , 0.8174517 , 0.8364954 ,\n",
      "       0.7964853 , 0.80312544, 0.8236924 , 0.85365325, 0.82128865,\n",
      "       0.84848213, 0.814654  , 0.7475145 , 0.8265485 , 0.84550613,\n",
      "       0.851411  , 0.80913717, 0.83243775, 0.8552901 , 0.83494157,\n",
      "       0.8597815 , 0.81348485, 0.828193  , 0.8595166 , 0.82089365,\n",
      "       0.80035096, 0.8105915 , 0.83044237, 0.7860274 , 0.84874296,\n",
      "       0.86732656, 0.8592988 , 0.8445199 , 0.83509517, 0.8436361 ,\n",
      "       0.80159813, 0.85218996, 0.80303645, 0.8264176 , 0.81043625,\n",
      "       0.83952856, 0.8384393 , 0.8328073 , 0.80141497, 0.8160043 ,\n",
      "       0.8688465 , 0.84838647, 0.86651164, 0.81822026, 0.81373334,\n",
      "       0.8430862 , 0.8415479 , 0.82399577, 0.832109  , 0.8360848 ,\n",
      "       0.83807963, 0.83003736, 0.8252947 , 0.8345586 , 0.8830496 ,\n",
      "       0.8245043 , 0.8420187 , 0.8332085 , 0.82860965, 0.82168025,\n",
      "       0.8132533 , 2.8845515 , 0.8198943 , 0.8123329 , 0.84799695,\n",
      "       0.8246986 , 0.84217644, 0.8453856 , 0.82103765, 0.8418659 ,\n",
      "       0.8065408 , 0.81762433, 0.82200086], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(lm_block.get_layer(index=1).get_weights())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.00458032, -0.02573658,  0.02269527, ...,  0.02642405,\n",
      "        -0.01966409, -0.02274757],\n",
      "       [ 0.01214657, -0.02762798, -0.03058223, ...,  0.01804959,\n",
      "         0.00809181, -0.00726894],\n",
      "       [-0.03293017,  0.00669291,  0.02711615, ..., -0.01748941,\n",
      "        -0.0174475 ,  0.0022877 ],\n",
      "       ...,\n",
      "       [ 0.01918899,  0.0020614 ,  0.03002295, ..., -0.01138652,\n",
      "        -0.00483538, -0.00028231],\n",
      "       [-0.02834468,  0.01324727,  0.02967318, ..., -0.01023217,\n",
      "        -0.02230434, -0.01218567],\n",
      "       [-0.03690905,  0.02856971,  0.02101301, ..., -0.01279723,\n",
      "        -0.00109855,  0.03700184]], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([[-0.00085744, -0.01362677,  0.0121907 , ...,  0.01864001,\n",
      "         0.03681674, -0.01593674],\n",
      "       [ 0.03150414, -0.01039497,  0.03417247, ..., -0.00574265,\n",
      "         0.00591145,  0.02696376],\n",
      "       [ 0.00651057,  0.00752371,  0.00105783, ...,  0.00465839,\n",
      "         0.03282674,  0.00777529],\n",
      "       ...,\n",
      "       [-0.01804837, -0.00035207,  0.02483237, ...,  0.03613684,\n",
      "         0.0370664 , -0.02029421],\n",
      "       [ 0.01073079, -0.01561692, -0.01196867, ..., -0.01625564,\n",
      "         0.02313478, -0.01031024],\n",
      "       [ 0.00542404, -0.01408278,  0.03836227, ...,  0.03807965,\n",
      "        -0.02393989, -0.03429682]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.], dtype=float32), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1.], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(lm_block.get_layer(index=2).get_weights())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "lm_block_layer3 = lm_block.get_layer(index=2)\n",
    "bert_layer12 = model.get_layer(index=12)\n",
    "bert_layer15 = model.get_layer(index=15)\n",
    "lm_block_layer3_weight = lm_block_layer3.get_weights()\n",
    "bert_layer12_weight = bert_layer12.get_weights()\n",
    "bert_layer15_weight = bert_layer15.get_weights()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bert_layer12_weight)):\n",
    "    print(lm_block_layer3_weight[i].shape == bert_layer12_weight[i].shape)\n",
    "for i in range(len(bert_layer15_weight)):\n",
    "    print(lm_block_layer3_weight[i+4].shape == bert_layer15_weight[i].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "6"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_weight = bert_layer12_weight + bert_layer15_weight\n",
    "len(combined_weight)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "lm_block_layer3.set_weights(combined_weight)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.0058011 ,  0.05985398, -0.03170604, ...,  0.06887805,\n",
      "         0.02423428, -0.03973034],\n",
      "       [-0.007228  , -0.02539035, -0.00167863, ...,  0.01907857,\n",
      "         0.01626231, -0.01045543],\n",
      "       [-0.04602809,  0.03800549, -0.00385575, ..., -0.02603067,\n",
      "        -0.06047573, -0.00679531],\n",
      "       ...,\n",
      "       [ 0.00474144,  0.10876587, -0.01808572, ...,  0.04912699,\n",
      "        -0.0229429 ,  0.00875449],\n",
      "       [-0.02682151, -0.10051435, -0.00056017, ..., -0.05286248,\n",
      "        -0.04268262, -0.03293934],\n",
      "       [-0.00666132,  0.02399873,  0.00313861, ..., -0.10766514,\n",
      "         0.09482101,  0.0334359 ]], dtype=float32), array([-0.03333926, -0.15175405,  0.03061958, ..., -0.12057822,\n",
      "       -0.08789343, -0.11828085], dtype=float32), array([[-0.04991841,  0.0067661 ,  0.00231446, ...,  0.04711771,\n",
      "         0.07601267, -0.04818509],\n",
      "       [ 0.03975736, -0.0166433 ,  0.0217378 , ..., -0.00293149,\n",
      "        -0.03669158, -0.05077926],\n",
      "       [ 0.02046659,  0.0257666 ,  0.06818264, ..., -0.0262746 ,\n",
      "        -0.01182084, -0.00768102],\n",
      "       ...,\n",
      "       [ 0.0340524 , -0.001765  , -0.01572011, ...,  0.02583974,\n",
      "        -0.00234455, -0.0897011 ],\n",
      "       [-0.00429181, -0.02082137, -0.02601478, ...,  0.04698761,\n",
      "        -0.00358716,  0.02460082],\n",
      "       [ 0.01654118,  0.00270375,  0.05313111, ...,  0.05192794,\n",
      "        -0.04381315, -0.00618155]], dtype=float32), array([ 0.07732435, -0.03586992, -0.01112988, -0.06081314,  0.11482707,\n",
      "       -0.04147998,  0.09477067, -0.07078534,  0.10520952, -0.08855211,\n",
      "       -0.0938862 , -0.00988956, -0.00600733, -0.14480066,  0.07875372,\n",
      "        0.04200479,  0.04227073,  0.01359683, -0.00742072,  0.04949265,\n",
      "        0.03048707,  0.12541398,  0.04234846, -0.03323954, -0.0625758 ,\n",
      "        0.03328647, -0.07015721,  0.00511542,  0.03767982,  0.02749067,\n",
      "       -0.11776352, -0.15779582, -0.00683794,  0.11254866,  0.09364499,\n",
      "       -0.01202308, -0.09632311,  0.01233637, -0.04145596,  0.07007767,\n",
      "       -0.09447882, -0.04723887, -0.07591319, -0.00513874,  0.10954606,\n",
      "        0.03161095,  0.0477336 , -0.00985719,  0.04328136,  0.02884118,\n",
      "        0.06621323,  0.18011917, -0.08994803,  0.1590338 ,  0.04133929,\n",
      "       -0.02942244, -0.05950917, -0.05797772,  0.01364362,  0.01003777,\n",
      "        0.06225392, -0.05958152,  0.14557524,  0.03819612, -0.06424975,\n",
      "        0.06002701, -0.0959563 ,  0.11185464, -0.05683229,  0.10059726,\n",
      "        0.06523568,  0.04116392, -0.11370601, -0.06270245,  0.07678126,\n",
      "        0.0813614 , -0.05908689, -0.03538384,  0.2129769 ,  0.03309944,\n",
      "       -0.04566747,  0.01005493,  0.07096435,  0.00980379,  0.13060364,\n",
      "       -0.0421106 , -0.02566227, -0.226412  , -0.06139677,  0.02761449,\n",
      "        0.021744  , -0.02803626,  0.09966734, -0.05185644, -0.02636505,\n",
      "       -0.03738423, -0.08332039,  0.03104243,  0.01544413, -0.03587195,\n",
      "       -0.03267491,  0.03493569,  0.07478776, -0.04239344,  0.20738393,\n",
      "        0.14715143,  0.01816223, -0.06843068,  0.01626879,  0.04556819,\n",
      "       -0.11353637,  0.02770445,  0.00798942,  0.07375111, -0.00906899,\n",
      "       -0.03354373, -0.06885434,  0.01909362, -0.0119425 ,  0.10184907,\n",
      "       -0.04400244, -0.05562102, -0.12009169,  0.11064151, -0.05947436,\n",
      "       -0.05096945, -0.03572518, -0.0384271 , -0.054387  , -0.04172289,\n",
      "        0.04197172, -0.04889184, -0.05641685, -0.00626792,  0.07207905,\n",
      "       -0.04476912,  0.07051273, -0.04407908,  0.17791755,  0.00139923,\n",
      "       -0.04039709,  0.00630422,  0.02720988, -0.0233524 , -0.06675649,\n",
      "        0.08452047,  0.09042046, -0.03119447,  0.02039048,  0.01170786,\n",
      "       -0.0361999 , -0.01323556,  0.09845527, -0.00560531,  0.01581238,\n",
      "       -0.12072805,  0.05780252, -0.09821353, -0.05481447, -0.01080853,\n",
      "        0.00415171,  0.00233685, -0.02945979, -0.02389897,  0.00827701,\n",
      "       -0.0264383 , -0.06570876,  0.00900321,  0.08804298,  0.08900189,\n",
      "       -0.07392802, -0.01542326,  0.02153572, -0.06252711, -0.01117808,\n",
      "       -0.02617737,  0.00931631,  0.00879904,  0.09130821,  0.01739606,\n",
      "        0.02091117,  0.01762321,  0.06730867,  0.10665402,  0.02063439,\n",
      "        0.08954784, -0.0294993 , -0.12346246, -0.09698336,  0.0201256 ,\n",
      "        0.02879921,  0.10639011, -0.0754546 ,  0.09579223,  0.01188174,\n",
      "        0.12019649,  0.2322432 , -0.03701728, -0.05727418, -0.045657  ,\n",
      "       -0.11525824, -0.01793344,  0.12585816, -0.03833813, -0.0156384 ,\n",
      "        0.14692584,  0.08406632,  0.01459419,  0.04578999,  0.07934339,\n",
      "        0.0097596 ,  0.0230484 ,  0.08748996, -0.04886137, -0.05782489,\n",
      "       -0.1450583 , -0.04596007, -0.03718168,  0.12415578,  0.0135073 ,\n",
      "        0.00901725, -0.06071623, -0.01303272, -0.04269099, -0.13509887,\n",
      "       -0.02708627,  0.06044169,  0.18459614,  0.07312667,  0.02347852,\n",
      "        0.05801546, -0.03874688,  0.01739662, -0.0910494 ,  0.05103343,\n",
      "       -0.08483926,  0.11095749, -0.05889035, -0.03630545,  0.05234295,\n",
      "        0.00484143,  0.005577  ,  0.07149301, -0.03502186, -0.02572532,\n",
      "       -0.02468468, -0.0604687 ,  0.07551055, -0.00843777, -0.0138734 ,\n",
      "       -0.01483663,  0.03926247,  0.06165963, -0.16385761,  0.01737848,\n",
      "        0.01537244, -0.04372871, -0.03753011,  0.06094794, -0.05342874,\n",
      "        0.0792058 ,  0.06373052, -0.1125812 ,  0.10480537,  0.09521942,\n",
      "       -0.03880996, -0.05103539, -0.04170556,  0.03720978, -0.00321378,\n",
      "       -0.02882485, -0.10101152,  0.00620275, -0.09273714, -0.00632074,\n",
      "        0.02731912, -0.04610743, -0.031157  , -0.01452493,  0.04019178,\n",
      "        0.00559257, -0.02507371,  0.09733897,  0.01170863, -0.00753176,\n",
      "       -0.11704759, -0.017849  ,  0.01983427,  0.00103781, -0.08428551,\n",
      "       -0.06714085, -0.01652999, -0.02732241,  0.07113533,  0.19674861,\n",
      "        0.04493755, -0.12049372,  0.03898561,  0.04224288,  0.00841292,\n",
      "       -0.04032006,  0.0587187 ,  0.01915855,  0.03033049,  0.13845144,\n",
      "       -0.01986931,  0.01149496, -0.01147906,  0.07916576, -0.02090352,\n",
      "       -0.0278885 , -0.08631532, -0.02132044, -0.05707959,  0.09379324,\n",
      "        0.11897329, -0.00731746,  0.11093987,  0.04247338, -0.07766869,\n",
      "        0.02349488, -0.04999957,  0.05084421,  0.10334513,  0.03763494,\n",
      "       -0.10489181,  0.09756343, -0.09934749,  0.03205002,  0.05149993,\n",
      "        0.08890395, -0.07132091,  0.02851513, -0.02029618, -0.08795538,\n",
      "       -0.04453923,  0.0872178 , -0.04305851,  0.02893931, -0.19533467,\n",
      "       -0.07735107, -0.02810275, -0.091276  , -0.09924924,  0.07215382,\n",
      "        0.07890975,  0.04665462,  0.03800513,  0.04192919,  0.01296534,\n",
      "        0.08673342, -0.00688454,  0.04158042,  0.14273165,  0.08670408,\n",
      "        0.04062784,  0.07576236,  0.0347856 ,  0.04086576, -0.15827438,\n",
      "       -0.08202289, -0.11701034,  0.08462913, -0.08439471,  0.07780383,\n",
      "        0.06554273, -0.0651351 , -0.00424457, -0.11993524, -0.14350703,\n",
      "        0.03979988,  0.05393756,  0.00857376, -0.1668303 , -0.07226235,\n",
      "        0.02796184, -0.0294693 , -0.03123634, -0.03760976, -0.0158098 ,\n",
      "       -0.08702362, -0.03855418, -0.01781338, -0.02263731, -0.09611431,\n",
      "        0.06949649, -0.0216132 ,  0.04340422, -0.1405528 , -0.01081519,\n",
      "       -0.00282117, -0.06792348, -0.02870765,  0.03006136,  0.07152727,\n",
      "       -0.00713785,  0.01403818,  0.08443464, -0.08598773, -0.01971199,\n",
      "       -0.01373733, -0.0777526 ,  0.01565039,  0.08809769, -0.07300008,\n",
      "        0.08838407,  0.11048575, -0.01912259,  0.01129887,  0.02760604,\n",
      "       -0.01055011, -0.08709501,  0.1388434 , -0.04028602,  0.00503002,\n",
      "        0.008476  , -0.01796337, -0.02111082,  0.14770776,  0.05145288,\n",
      "       -0.04143635, -0.06197591, -0.03946346,  0.01726036,  0.07745652,\n",
      "        0.02825372,  0.04217833, -0.01117395,  0.01037786,  0.05295409,\n",
      "       -0.02928282,  0.08430524,  0.07727465,  0.06073789, -0.11084223,\n",
      "       -0.0792525 ,  0.05798435,  0.11928851, -0.05263995, -0.07378739,\n",
      "        0.11096931, -0.10021663,  0.00518228,  0.06518757,  0.02333356,\n",
      "       -0.00947199,  0.02294865,  0.01589355, -0.06423134, -0.05565627,\n",
      "        0.065473  ,  0.05270598,  0.0431456 ,  0.03059968,  0.02170243,\n",
      "       -0.00664273, -0.06967781,  0.04329848, -0.02046579, -0.1877997 ,\n",
      "       -0.18088101,  0.03126482,  0.04301506,  0.13398796,  0.047678  ,\n",
      "       -0.00753494,  0.06104536,  0.00151123,  0.08034955,  0.04667015,\n",
      "        0.0309238 , -0.03940424,  0.07466351, -0.09937181, -0.04449298,\n",
      "        0.00313627, -0.05758741,  0.02132616, -0.06373314, -0.03902182,\n",
      "       -0.02783875,  0.02536687,  0.0046314 ,  0.02389102, -0.07440458,\n",
      "        0.01239118, -0.11852416, -0.14698336,  0.05806556,  0.01191499,\n",
      "       -0.05927712,  0.08977584,  0.06355482, -0.03134774,  0.00439951,\n",
      "       -0.11215127, -0.06846584,  0.00150376,  0.00941292, -0.07108589,\n",
      "       -0.10642095,  0.04558153,  0.02004034, -0.06981782, -0.02269331,\n",
      "        0.05309162,  0.2749527 ,  0.08112594,  0.07295446,  0.05828891,\n",
      "        0.00787562,  0.04622251,  0.03234083,  0.01097141,  0.05377483,\n",
      "       -0.00592299, -0.01312505, -0.00423891,  0.15944543, -0.12437742,\n",
      "       -0.00339121,  0.01167093, -0.0113735 , -0.1274974 , -0.06199693,\n",
      "        0.06301951,  0.05215719, -0.23531666,  0.03817869,  0.05719974,\n",
      "        0.05735907, -0.03565398, -0.06586105,  0.03481732, -0.08371074,\n",
      "       -0.10578264,  0.00687771, -0.15628436,  0.10668135,  0.00243496,\n",
      "       -0.12739423,  0.01320765,  0.03708578,  0.05216132, -0.03523632,\n",
      "       -0.0955798 , -0.0047453 , -0.01775523,  0.03392892, -0.06897674,\n",
      "        0.09858214, -0.0614261 , -0.10273883, -0.01046638,  0.08701999,\n",
      "       -0.07607053,  0.01688968, -0.02449217,  0.02451652, -0.06300889,\n",
      "        0.06922267, -0.00880181,  0.00414248, -0.00510904,  0.10261123,\n",
      "        0.02784323,  0.01371373,  0.09519401,  0.01681131, -0.0705351 ,\n",
      "        0.01551499, -0.06371316,  0.0349979 , -0.11712932, -0.0136661 ,\n",
      "       -0.01861681, -0.13088477,  0.03428392,  0.03119572, -0.03124343,\n",
      "       -0.01080509,  0.01817144,  0.01772532, -0.04966091,  0.03568536,\n",
      "        0.05309092,  0.02674834,  0.02577469, -0.0400046 , -0.20720111,\n",
      "       -0.01491701, -0.01236863,  0.04699149, -0.01676076, -0.04546803,\n",
      "        0.04361048,  0.01083045, -0.14625128,  0.05100689, -0.00442998,\n",
      "       -0.12413818,  0.0542662 , -0.02319281, -0.07225315, -0.01458919,\n",
      "        0.0636049 ,  0.02139633, -0.09069385,  0.08769462, -0.09256831,\n",
      "        0.04058283, -0.14808673, -0.03435963, -0.02712739,  0.11050294,\n",
      "       -0.02968722,  0.00038029,  0.06704661,  0.03564033,  0.0064029 ,\n",
      "        0.04196899, -0.03880643, -0.02418878, -0.02178664, -0.13110554,\n",
      "        0.14061408, -0.02801099, -0.06647146,  0.04643597, -0.17834333,\n",
      "        0.04940677, -0.01187214, -0.1394251 , -0.12549321,  0.0710194 ,\n",
      "       -0.02491528, -0.04996925,  0.169227  , -0.04418142, -0.03172163,\n",
      "        0.01572775, -0.10936587, -0.01222678, -0.00074178,  0.02257887,\n",
      "        0.03673593,  0.02959156, -0.02612534,  0.11113957,  0.16623779,\n",
      "        0.082451  ,  0.00302219,  0.10868563, -0.15227297,  0.10494299,\n",
      "       -0.01792176, -0.00198996,  0.0185532 , -0.06731869,  0.00604167,\n",
      "        0.14412795,  0.04072466, -0.02756977, -0.05097365,  0.07333537,\n",
      "       -0.03076723,  0.02165941,  0.02015908,  0.09639724, -0.03379638,\n",
      "        0.00257506, -0.07226729, -0.01354803, -0.01776151, -0.01205341,\n",
      "       -0.01550599,  0.01201273, -0.06739171,  0.04555395, -0.0189404 ,\n",
      "       -0.02503014,  0.08641353, -0.05086133,  0.1391539 , -0.04830345,\n",
      "        0.06594048,  0.07069267,  0.04777324,  0.01252819, -0.04385685,\n",
      "       -0.04960655,  0.06392776, -0.01249609, -0.07091594,  0.06415093,\n",
      "       -0.03948295,  0.07329349,  0.05855072, -0.09061887,  0.11926384,\n",
      "       -0.02020675,  0.09334761,  0.10256741, -0.08942127, -0.00373754,\n",
      "        0.01354909, -0.0428335 , -0.02000259, -0.05460678,  0.0482768 ,\n",
      "        0.06700914, -0.00815748, -0.05809419,  0.10234177, -0.05415493,\n",
      "       -0.06506325, -0.01577814, -0.03495541,  0.09337324, -0.08924012,\n",
      "        0.03655949, -0.00504776,  0.07513548, -0.01692071,  0.01659897,\n",
      "       -0.06129806,  0.03610742,  0.07153912, -0.03348562, -0.0391959 ,\n",
      "       -0.00417974, -0.0041965 ,  0.11151931, -0.02563469, -0.08316701,\n",
      "       -0.0955212 ,  0.1276383 , -0.16876301,  0.01671331,  0.14439337,\n",
      "        0.05471106, -0.04594819, -0.04572589,  0.03258999, -0.11471195,\n",
      "        0.09153293, -0.01921054,  0.01552523,  0.04062653, -0.01831373,\n",
      "        0.07444049, -0.12844396,  0.02447071, -0.04626526,  0.0476754 ,\n",
      "       -0.01244076, -0.11455373,  0.02299694, -0.08890011,  0.04153472,\n",
      "       -0.17092264,  0.03912232, -0.14767776, -0.18636256,  0.0538422 ,\n",
      "        0.03788407,  0.02921471, -0.12824488], dtype=float32), array([ 1.08947400e-02, -3.50601114e-02, -3.93857062e-02, -6.05298299e-03,\n",
      "       -1.38286084e-01,  1.28665268e-02, -2.55525280e-02, -5.44271842e-02,\n",
      "        2.84506734e-02, -1.76003315e-02, -8.26675147e-02,  7.58797675e-02,\n",
      "        6.74191490e-02, -4.96447145e-04, -2.47977190e-02,  7.00600445e-03,\n",
      "       -1.67664550e-02, -3.93222366e-03, -6.82054758e-02, -4.10231687e-02,\n",
      "       -3.57662737e-02, -5.16304746e-03, -2.92803552e-02,  5.09773046e-02,\n",
      "       -2.73223240e-02, -1.58547405e-02, -3.55472304e-02,  2.27774326e-02,\n",
      "        2.94218007e-02, -1.04255714e-01, -8.94506350e-02, -1.48016596e-02,\n",
      "       -1.52233131e-02,  6.68491200e-02, -2.12611854e-02, -1.34039164e-01,\n",
      "       -3.98902148e-02,  1.71446148e-02, -3.22409086e-02,  4.39096093e-02,\n",
      "       -2.24101115e-02, -3.74407023e-02, -6.84259161e-02,  1.94781590e-02,\n",
      "       -3.11650466e-02, -2.60436069e-03,  1.59263983e-02, -1.98581517e-02,\n",
      "       -1.02233356e-02,  8.65960401e-03,  5.08440891e-03, -1.49800926e-01,\n",
      "       -4.59801070e-02,  2.10415982e-02, -4.65844162e-02, -1.43917492e-02,\n",
      "       -9.15495097e-04, -5.16454969e-03, -1.03580147e-01, -1.20957950e-02,\n",
      "        4.64594960e-02, -3.19252834e-02,  2.94583756e-02, -2.66536791e-02,\n",
      "       -2.20428929e-02,  3.59553583e-02,  9.46626142e-02, -2.65478566e-02,\n",
      "       -2.61010025e-02,  3.29881422e-02,  8.96202400e-03,  1.62756629e-02,\n",
      "       -2.37911493e-02, -5.88478856e-02,  2.59378180e-02,  5.19234277e-02,\n",
      "       -2.05259696e-02, -2.48546060e-03,  6.60743266e-02,  4.80297804e-02,\n",
      "       -1.41513795e-02,  7.79361464e-03,  3.37251648e-02,  2.32337322e-02,\n",
      "       -5.99863194e-03,  1.18505349e-02, -2.51055993e-02,  2.37023637e-01,\n",
      "        2.56018750e-02, -4.35434505e-02,  1.96776260e-03,  3.90658043e-02,\n",
      "       -1.36209223e-02,  1.59640126e-02,  8.71019997e-03,  6.29554167e-02,\n",
      "       -1.63095184e-02,  2.59459373e-02,  1.21913720e-02, -1.47127290e-03,\n",
      "       -1.96279325e-02,  4.68093120e-02,  4.26905304e-02, -9.13062133e-03,\n",
      "        2.93723065e-02,  8.66061728e-03, -6.84226826e-02, -1.32886032e-02,\n",
      "       -4.95415442e-02,  1.67099182e-02, -8.45791101e-02,  2.37660389e-02,\n",
      "        1.11096036e-02, -3.10927033e-02, -7.06839412e-02,  3.79738919e-02,\n",
      "       -9.59301181e-03,  3.35415751e-02, -3.57568227e-02,  1.13916891e-02,\n",
      "       -1.58109237e-02, -3.76849659e-02, -9.31554511e-02,  4.49002674e-03,\n",
      "       -7.98244774e-03, -4.51143309e-02, -2.12513264e-02,  4.19232287e-02,\n",
      "       -1.55514488e-02,  1.83703639e-02,  1.24345841e-02, -3.21476301e-03,\n",
      "       -6.34007230e-02, -2.86024474e-02,  8.29691738e-02, -3.31074148e-02,\n",
      "        2.94289589e-02, -4.49820273e-02,  5.40734157e-02, -7.72964284e-02,\n",
      "       -2.24064644e-02,  6.36165589e-03,  5.76725788e-02, -4.62273583e-02,\n",
      "       -1.04140520e-01,  2.01858650e-03, -3.30771878e-02, -2.30722912e-02,\n",
      "       -4.74979542e-02, -4.01486568e-02, -4.33413237e-02, -2.70812418e-02,\n",
      "       -6.85644820e-02,  5.94552495e-02, -5.99979842e-03, -2.48322971e-02,\n",
      "        4.03930172e-02, -1.09682851e-01,  2.54787114e-02,  2.84543224e-02,\n",
      "        1.03968270e-02,  7.91108236e-04, -1.33580789e-02, -6.17260784e-02,\n",
      "        2.17948556e-02,  3.03050354e-02, -2.91549526e-02, -5.86083643e-02,\n",
      "        3.19526270e-02,  3.54830362e-02, -1.02392614e-01, -6.49367273e-03,\n",
      "        1.19633209e-02, -9.07580331e-02, -5.32619804e-02, -8.43846332e-03,\n",
      "       -8.99241343e-02,  2.44810507e-02,  1.91401675e-01,  5.79646183e-03,\n",
      "        7.24730967e-03, -1.42888119e-02, -5.32576479e-02, -6.29162565e-02,\n",
      "       -3.44620831e-02,  5.20551912e-02, -2.06089523e-02, -6.75527304e-02,\n",
      "        1.16979808e-01,  1.85792893e-02,  4.83921617e-02,  4.67146710e-02,\n",
      "        4.65903580e-02,  2.15148125e-02, -2.15902552e-02,  1.29224181e-01,\n",
      "       -1.02728598e-01, -2.34441068e-02, -1.29474734e-03, -4.67222184e-02,\n",
      "        1.41566200e-02,  2.19856277e-02, -4.00187895e-02, -7.14449197e-05,\n",
      "        6.82733080e-04, -3.53082526e-03, -2.05705743e-02, -1.48333388e-03,\n",
      "       -1.67666990e-02,  1.66948840e-01,  3.58587652e-02,  2.97230203e-02,\n",
      "        7.48408362e-02, -5.37857367e-03,  2.49200370e-02,  1.56271718e-02,\n",
      "        4.57147062e-02, -1.93289351e-02,  1.68889035e-02, -6.73611742e-03,\n",
      "       -9.43435356e-03, -2.75982209e-02,  5.13622724e-03,  6.25179857e-02,\n",
      "       -3.50806378e-02, -1.03240041e-02, -1.04534877e-02,  5.67207225e-02,\n",
      "       -1.02787809e-02,  3.15161943e-02, -4.16092165e-02,  7.16237575e-02,\n",
      "        2.54588574e-02,  8.05970654e-02,  1.72844324e-02, -6.42955527e-02,\n",
      "       -3.58066820e-02, -2.16250122e-02,  3.75200473e-02, -1.61225796e-02,\n",
      "       -5.71805611e-02,  1.23764277e-01, -6.81968406e-02,  6.27396256e-02,\n",
      "        8.07138626e-03,  5.05628483e-03,  2.33004019e-02,  6.35196418e-02,\n",
      "        1.11954287e-02, -2.42810398e-02, -3.95900495e-02, -5.22998646e-02,\n",
      "       -3.59030813e-02, -1.02834208e-02, -3.27502166e-05,  1.50162382e-02,\n",
      "       -3.46542597e-02, -5.10666631e-02, -1.77845675e-02, -3.90631780e-02,\n",
      "       -2.55743027e-01, -5.67266606e-02,  9.50251240e-03,  4.98510636e-02,\n",
      "       -4.36675884e-02,  3.13115530e-02,  2.07614508e-02,  1.14015369e-02,\n",
      "        4.07233611e-02, -4.04769741e-02,  1.91108696e-02, -9.52597782e-02,\n",
      "       -2.17400827e-02,  5.73899504e-03, -2.97275502e-02, -1.40493261e-02,\n",
      "       -1.05330490e-01, -3.13517712e-02,  3.66711267e-03,  9.95580256e-02,\n",
      "       -9.70786810e-03, -3.25509049e-02, -2.32906789e-02,  6.26004264e-02,\n",
      "       -1.01825176e-02, -3.28240283e-02, -2.25375150e-03, -1.99247487e-02,\n",
      "       -2.04379801e-04, -3.10362298e-02, -2.99597420e-02, -5.20584844e-02,\n",
      "       -1.09271910e-02,  3.34926844e-02, -3.76941830e-01, -1.90595333e-02,\n",
      "       -1.16647277e-02,  4.22129929e-02, -6.02152348e-02, -1.52606824e-02,\n",
      "       -6.60959482e-02, -3.69826145e-02,  3.14511731e-02,  3.71779948e-02,\n",
      "        4.94352691e-02,  2.71794796e-02, -3.95293497e-02, -3.29706781e-02,\n",
      "        4.26660776e-02, -7.56025314e-02,  3.07391188e-03,  8.57105292e-03,\n",
      "       -3.30196209e-02, -2.41150204e-02,  2.72644069e-02,  6.52926192e-02,\n",
      "       -5.63710369e-02,  5.28079830e-02, -5.02167605e-02, -2.96672508e-02,\n",
      "       -5.49615063e-02,  1.93094760e-02, -3.42960916e-02,  2.44632661e-02,\n",
      "       -6.64056391e-02, -3.70546803e-02, -4.75319438e-02, -1.10971123e-01,\n",
      "       -3.21204495e-03, -1.33530907e-02,  4.86350469e-02, -6.69426247e-02,\n",
      "        1.49805183e-02,  2.11005267e-02, -9.22562089e-03, -4.03227769e-02,\n",
      "        1.53625868e-02, -7.18034431e-02, -7.83179328e-02, -5.26713207e-02,\n",
      "       -2.10661404e-02, -3.05259060e-02, -3.65510285e-02,  6.12785993e-03,\n",
      "       -3.23779471e-02,  5.06225452e-02, -9.92260873e-03,  5.73048890e-02,\n",
      "        2.01410074e-02, -1.37793124e-02,  7.14012608e-02,  5.16883880e-02,\n",
      "       -4.43004593e-02, -5.24183698e-02,  4.47531827e-02,  6.77736327e-02,\n",
      "        2.91060694e-02, -5.92901977e-03, -3.00480593e-02, -1.56586450e-02,\n",
      "       -1.41893364e-02, -8.96627605e-02,  2.56767850e-02, -9.51263681e-02,\n",
      "        1.79024667e-01,  4.27717455e-02, -3.69376317e-02, -6.84609860e-02,\n",
      "       -2.01667030e-03,  1.88120995e-02, -8.73945421e-04,  8.77641216e-02,\n",
      "       -2.18013059e-02, -1.19317211e-02,  4.09777761e-02,  5.14624007e-02,\n",
      "        6.47996664e-02,  3.47689018e-02,  8.80306121e-03,  1.42036900e-02,\n",
      "       -1.02072228e-02,  4.71785069e-02, -2.64449213e-02, -7.44510721e-03,\n",
      "       -7.59087875e-02, -1.17253885e-02,  2.66455915e-02, -6.62046717e-03,\n",
      "        8.36841315e-02, -4.42110263e-02, -1.85751822e-02, -6.45344853e-02,\n",
      "       -1.05714716e-01, -2.73172818e-02, -7.08529949e-02, -2.43570376e-03,\n",
      "        4.93343212e-02,  2.49139480e-02, -2.04330366e-02, -1.96814793e-03,\n",
      "        2.87415590e-02,  1.39208687e-02, -5.55993281e-02, -5.09946048e-02,\n",
      "       -6.32572025e-02,  2.58585569e-02,  1.98741145e-02, -3.81826051e-02,\n",
      "       -2.10647588e-03, -4.28241082e-02,  7.88633525e-02, -2.17013899e-02,\n",
      "        7.34642940e-03, -3.53964530e-02, -5.58515592e-03,  2.18113996e-02,\n",
      "        3.48443016e-02,  5.68755902e-03,  6.22265898e-02,  4.53449748e-02,\n",
      "       -4.92197350e-02, -1.23892352e-01,  8.04838259e-03,  2.08874451e-04,\n",
      "        1.33657996e-02,  2.08849590e-02,  2.38106418e-02, -3.86194438e-02,\n",
      "        3.95950209e-03, -2.30089575e-02, -9.58508253e-03, -1.53834261e-02,\n",
      "       -1.39159104e-02,  5.69562148e-03, -5.15191481e-02, -4.51185944e-04,\n",
      "       -2.55684312e-02,  5.12250401e-02, -3.51816416e-02, -3.82008888e-02,\n",
      "       -1.75440088e-02, -5.21001406e-02,  2.60852892e-02, -1.82030834e-02,\n",
      "       -6.04754947e-02,  2.76524555e-02,  7.62161463e-02,  6.97076097e-02,\n",
      "       -5.58152236e-02, -2.95277052e-02, -3.51575390e-02, -5.01512587e-02,\n",
      "        7.45563731e-02,  2.10167076e-02, -3.20119481e-03, -3.87347229e-02,\n",
      "        1.49148870e-02,  1.66339092e-02, -4.19233181e-02,  2.25583538e-02,\n",
      "       -3.89361084e-02,  1.38079952e-02, -9.23010334e-03,  2.25138236e-02,\n",
      "       -6.99628592e-02,  1.10502588e-02, -2.05712696e-03, -3.52020711e-02,\n",
      "       -4.94464450e-02,  1.39904454e-01,  4.01951722e-04, -1.28959036e-02,\n",
      "        1.95212197e-02, -9.50260833e-03,  9.93290171e-02, -9.60602611e-03,\n",
      "        1.34062627e-02,  6.82284171e-03, -7.22857639e-02,  6.58866316e-02,\n",
      "       -4.73085493e-02, -4.70553637e-02, -6.68586977e-03, -1.81189980e-02,\n",
      "       -5.27369343e-02, -1.57580823e-02, -1.51623730e-02,  2.84188949e-02,\n",
      "        5.73728718e-02, -3.24714147e-02, -2.43039406e-03,  1.25471288e-02,\n",
      "        5.43044284e-02, -4.44056764e-02,  6.22388944e-02, -7.38776103e-02,\n",
      "        2.48199757e-02, -5.13777079e-04,  2.81844009e-02,  8.50347728e-02,\n",
      "       -5.19483350e-02,  4.38223481e-02, -1.74766481e-02, -1.53686134e-02,\n",
      "       -3.13493274e-02, -4.80212346e-02,  1.39745576e-02,  1.21737853e-01,\n",
      "       -1.27311647e-02,  4.13417183e-02, -1.09275348e-01, -5.66377267e-02,\n",
      "       -1.05364842e-03, -2.98375208e-02, -1.19614676e-02, -1.32156080e-02,\n",
      "       -8.27301294e-02,  4.37857062e-02, -1.35775618e-02, -4.84341495e-02,\n",
      "       -1.82100423e-02, -1.47552928e-02, -3.72383446e-02, -5.31326085e-02,\n",
      "       -9.25945118e-03, -6.61398144e-03,  2.03554500e-02, -4.01928909e-02,\n",
      "        4.43055071e-02, -1.86990462e-02,  7.18537392e-03,  4.80773225e-02,\n",
      "       -1.21469144e-02, -4.87338640e-02, -5.73649444e-02, -1.77185144e-02,\n",
      "        7.12485090e-02,  2.46642195e-02,  3.33731016e-03, -4.61215563e-02,\n",
      "       -3.79549004e-02,  4.29891683e-02, -3.32492590e-02,  2.01299787e-02,\n",
      "        8.48553050e-03, -4.35041711e-02, -5.80167063e-02, -1.77928507e-02,\n",
      "        1.73292421e-02,  2.64678663e-03,  2.69592162e-02, -3.75962676e-03,\n",
      "       -8.07112828e-02,  8.85488559e-03,  1.18202660e-02, -2.49761511e-02,\n",
      "       -1.04706232e-02, -1.79068800e-02, -3.55739258e-02, -4.98791486e-02,\n",
      "        5.58836684e-02, -1.08202668e-02,  3.28543000e-02,  3.91344279e-02,\n",
      "        8.83660316e-02, -3.00372522e-02,  1.70778967e-02,  1.00438958e-02,\n",
      "        1.84810013e-02,  4.08457294e-02, -4.05191928e-02, -5.17650843e-02,\n",
      "        2.26459224e-02, -6.67386875e-02,  5.71154710e-03, -1.09288590e-02,\n",
      "        2.07707603e-02,  6.82627177e-03, -3.26640718e-02, -6.02498837e-02,\n",
      "       -2.82137729e-02, -3.03848111e-03, -6.31760657e-02, -3.04850582e-02,\n",
      "       -6.16854466e-02,  3.75146829e-02,  4.55806553e-02,  2.83543579e-02,\n",
      "        1.25805493e-02, -2.87426040e-02, -5.18758371e-02,  3.94342979e-03,\n",
      "       -2.09449641e-02,  5.10068014e-02, -2.22668909e-02, -8.48193374e-03,\n",
      "       -6.80622756e-02, -8.93919766e-02, -4.02466580e-02, -5.74818477e-02,\n",
      "       -1.09087653e-01,  3.17977443e-02, -3.57138999e-02, -4.98542786e-02,\n",
      "        1.54136242e-02, -3.70746441e-02, -2.03371141e-03, -8.48645866e-02,\n",
      "        5.73409274e-02, -4.57604751e-02,  3.44362743e-02, -3.61536592e-02,\n",
      "       -2.66829822e-02, -6.04029279e-03, -6.35779742e-03, -6.76360130e-02,\n",
      "        1.38602749e-01, -4.55924235e-02,  6.76765293e-02, -1.88146848e-02,\n",
      "        1.89727861e-02,  1.38885155e-02, -1.58203784e-02,  2.41600480e-02,\n",
      "       -1.81031879e-02,  2.50493269e-02, -6.61164448e-02,  2.18746252e-02,\n",
      "       -1.81776285e-02,  5.62274305e-04,  5.46705686e-02, -2.20338535e-02,\n",
      "       -2.51015294e-02,  4.58383560e-03, -3.63681987e-02, -5.04042730e-02,\n",
      "       -3.08214668e-02,  1.30734131e-01, -4.32581864e-02, -8.76882523e-02,\n",
      "       -2.44106445e-02, -7.08782300e-02,  4.57928292e-02,  1.76974926e-02,\n",
      "       -1.56065095e-02,  4.69101593e-02,  3.60796507e-03,  1.15531817e-01,\n",
      "        9.02051330e-02,  7.43383588e-03,  3.26588228e-02, -2.46004127e-02,\n",
      "        5.52586541e-02,  9.06247739e-03,  5.81137054e-02, -1.70945525e-02,\n",
      "        1.16358977e-02,  1.38045503e-02, -1.62079968e-02, -1.39759090e-02,\n",
      "        4.34434488e-02, -6.62402716e-03,  8.97786755e-04, -4.34458479e-02,\n",
      "       -7.86142126e-02, -6.34763576e-03,  4.19174619e-02,  2.27923114e-02,\n",
      "       -3.39748003e-02, -5.64757288e-02,  3.78419086e-02,  6.89912289e-02,\n",
      "       -4.88506332e-02,  2.68763583e-02, -6.40443759e-03,  1.48387328e-02,\n",
      "       -2.55574193e-02, -1.47545792e-03, -6.68278709e-02,  2.48209443e-02,\n",
      "        2.63556205e-02,  2.05776896e-02, -2.20355932e-02, -1.51061662e-03,\n",
      "        4.54939902e-02, -2.03395989e-02,  1.68525241e-02,  5.04531413e-02,\n",
      "        9.14414972e-02, -7.97023177e-02,  6.29881769e-02,  5.80130331e-03,\n",
      "        5.93637750e-02,  1.48568861e-02, -1.76690426e-02,  2.06567347e-02,\n",
      "        1.00641642e-02,  2.14980002e-02, -5.13634980e-02,  6.71150768e-03,\n",
      "        2.53965091e-02, -6.05894532e-03,  6.94645196e-02, -4.48159035e-03,\n",
      "       -1.97339952e-02,  1.60387270e-02, -2.28515137e-02, -5.40909998e-04,\n",
      "       -3.01196817e-02, -3.55091430e-02,  2.76265498e-02, -4.51730266e-02,\n",
      "       -1.93833094e-02,  1.52873555e-02, -3.71322595e-02, -1.77733935e-02,\n",
      "        1.32737244e-02, -2.36453991e-02,  7.31012225e-02, -6.56392947e-02,\n",
      "       -1.29442234e-02, -3.18954065e-02, -1.74914896e-02,  2.98628621e-02,\n",
      "        1.56596974e-02, -8.92082304e-02, -6.48231339e-03,  6.38727024e-02,\n",
      "       -5.57621866e-02, -5.93443587e-02,  9.94326640e-03,  4.53790091e-02,\n",
      "        3.56159247e-02, -3.15007679e-02,  3.42444964e-02, -3.89154702e-02,\n",
      "        4.55098338e-02, -2.16844734e-02,  9.47735214e-04,  2.46414710e-02,\n",
      "       -7.41848126e-02, -5.96506633e-02, -8.75931606e-02,  4.92714520e-04,\n",
      "       -9.50572640e-02,  3.30008455e-02, -6.22770041e-02,  2.27220077e-02,\n",
      "        1.04910284e-02, -6.06797785e-02,  2.96804477e-02, -2.08464731e-02,\n",
      "       -1.90356467e-02,  3.19407531e-03, -2.90697012e-02, -1.21603478e-02,\n",
      "       -1.56977510e+00, -8.95993598e-03,  9.94116906e-03, -2.23657731e-02,\n",
      "       -5.37513979e-02, -5.06223440e-02, -2.98054572e-02,  1.29289838e-04,\n",
      "       -1.26046501e-02,  8.94365683e-02,  1.94778517e-02,  1.00256912e-02],\n",
      "      dtype=float32), array([0.9421534 , 0.9493645 , 0.93407094, 0.90346694, 0.6444572 ,\n",
      "       0.9177583 , 0.89003104, 0.9190424 , 0.85947883, 0.9625108 ,\n",
      "       0.9395849 , 0.9010652 , 0.74637103, 0.9120841 , 0.92137975,\n",
      "       0.90528125, 0.92670906, 0.9279048 , 0.9288691 , 0.92103636,\n",
      "       0.93958193, 0.92646146, 0.97496396, 0.9021086 , 0.91047764,\n",
      "       0.9440597 , 0.9365051 , 0.92591906, 0.9577324 , 0.93152314,\n",
      "       0.8802463 , 0.94779843, 0.9464007 , 0.9084593 , 0.963723  ,\n",
      "       0.92136854, 0.92962587, 0.9122404 , 0.92677474, 0.9472858 ,\n",
      "       0.9260731 , 0.93143463, 0.94491243, 0.9633955 , 0.9027997 ,\n",
      "       0.92977023, 0.9232748 , 0.9327065 , 0.92087394, 0.92584634,\n",
      "       0.92304194, 0.5406279 , 0.90906996, 0.90615284, 0.9113416 ,\n",
      "       0.9309821 , 0.9324039 , 0.9132261 , 0.8649345 , 0.96105075,\n",
      "       0.9592782 , 0.9201698 , 0.89545524, 0.90578175, 0.94993156,\n",
      "       0.9156826 , 0.9250341 , 0.91614467, 0.9020408 , 0.89793223,\n",
      "       0.9077867 , 0.9122607 , 0.9369463 , 0.92073   , 0.95203346,\n",
      "       0.89929557, 0.9209753 , 0.90718967, 0.7697049 , 0.9265482 ,\n",
      "       0.9299971 , 0.88262695, 0.8893766 , 0.9337411 , 0.9290426 ,\n",
      "       0.95648944, 0.9182301 , 0.94557595, 0.9065525 , 0.8983961 ,\n",
      "       0.89744973, 0.930466  , 0.92065173, 0.90396243, 0.898908  ,\n",
      "       0.45161632, 0.89738834, 0.9280699 , 0.92781395, 0.9103288 ,\n",
      "       0.6961561 , 0.92092264, 0.91100013, 0.9765622 , 0.9023795 ,\n",
      "       0.9041736 , 0.92841315, 0.9105107 , 0.9164594 , 0.9258975 ,\n",
      "       0.9418472 , 0.9163321 , 0.94161725, 0.93801427, 0.9099349 ,\n",
      "       0.91433865, 0.9515557 , 0.88887966, 0.92086434, 0.92756146,\n",
      "       0.9408041 , 0.94599795, 0.9142766 , 0.90822273, 0.9339241 ,\n",
      "       0.90332   , 0.9080537 , 0.9036317 , 0.94174665, 0.9262632 ,\n",
      "       0.9377704 , 0.9094404 , 0.9416131 , 0.90239316, 0.9253335 ,\n",
      "       0.9716334 , 0.9142868 , 0.9127098 , 0.9369021 , 0.8878044 ,\n",
      "       0.94626343, 0.9631875 , 0.93817073, 0.9006485 , 0.8872938 ,\n",
      "       0.91157323, 0.93479186, 0.95044696, 0.9043093 , 0.9189511 ,\n",
      "       0.9300971 , 0.8681638 , 0.88898295, 0.89569885, 0.905216  ,\n",
      "       0.9336028 , 0.9452199 , 0.9428986 , 0.92224985, 0.92585856,\n",
      "       0.925799  , 0.9092075 , 0.9426835 , 0.92521214, 0.9242661 ,\n",
      "       0.9368766 , 0.9618585 , 0.9055848 , 0.927342  , 0.9003321 ,\n",
      "       0.82198364, 0.9329716 , 0.9107772 , 0.90876424, 0.9167911 ,\n",
      "       0.93703866, 0.9297387 , 0.933016  , 0.6651605 , 0.97502744,\n",
      "       0.940634  , 0.95427185, 0.9310026 , 0.9094547 , 0.8705588 ,\n",
      "       0.94815725, 0.8986187 , 0.91352004, 0.6634819 , 0.9126022 ,\n",
      "       0.93875253, 0.916353  , 0.79510397, 0.91730875, 0.91229963,\n",
      "       0.58054984, 0.8479751 , 0.94163114, 0.8685265 , 0.9379592 ,\n",
      "       0.8907467 , 0.9100054 , 0.9439753 , 0.9469962 , 0.89214516,\n",
      "       0.93681437, 0.9501118 , 0.9005316 , 0.93374616, 0.7486573 ,\n",
      "       0.9263699 , 0.94104   , 0.9244851 , 0.9511369 , 0.9378406 ,\n",
      "       0.91973644, 0.93239594, 0.88095945, 0.91853267, 0.96528244,\n",
      "       0.90725696, 0.9320901 , 0.9360874 , 0.8567467 , 0.92224616,\n",
      "       0.9116512 , 0.92432326, 0.92727786, 0.8819577 , 0.93606347,\n",
      "       0.9511674 , 0.9097277 , 0.9247586 , 0.8713394 , 0.94509727,\n",
      "       0.9570579 , 0.9203571 , 0.90261513, 0.9137326 , 0.9240369 ,\n",
      "       0.9416038 , 0.68401325, 0.88945836, 0.63682646, 0.93474317,\n",
      "       0.90752965, 0.89114386, 0.94896024, 0.9563645 , 0.9257782 ,\n",
      "       0.9200984 , 0.92679715, 0.8627932 , 0.77089906, 0.92530984,\n",
      "       0.92794883, 0.9106006 , 0.895568  , 0.92965317, 0.9363937 ,\n",
      "       0.6564894 , 0.9435996 , 0.92123806, 0.7688738 , 0.9247638 ,\n",
      "       0.70278233, 0.92947537, 0.91693735, 0.9233053 , 0.9337403 ,\n",
      "       0.9365283 , 0.9031046 , 0.9367706 , 0.9207152 , 0.9350137 ,\n",
      "       0.9355223 , 0.9476251 , 0.93795913, 0.94834447, 0.9081468 ,\n",
      "       0.96213204, 0.92612875, 0.9585556 , 0.932316  , 0.89845395,\n",
      "       0.9245977 , 0.89132446, 0.9579955 , 0.8746367 , 0.9426728 ,\n",
      "       0.93886685, 0.9421855 , 0.65716684, 0.91846395, 0.2772973 ,\n",
      "       0.9075518 , 0.9579318 , 0.9575405 , 0.92307645, 0.96072155,\n",
      "       0.9368896 , 0.9268634 , 0.9187078 , 0.92216533, 0.8676683 ,\n",
      "       0.9533767 , 0.9273943 , 0.8645243 , 0.95421404, 0.92824996,\n",
      "       0.9135882 , 0.92789656, 0.9469113 , 0.7958013 , 0.92874503,\n",
      "       0.7701628 , 0.9179552 , 0.91328955, 0.91010046, 0.9611156 ,\n",
      "       0.96022815, 0.91986674, 0.9640219 , 0.95426494, 0.95764136,\n",
      "       0.90987146, 0.94111675, 0.9198511 , 0.9297798 , 0.91550106,\n",
      "       0.90453315, 0.94400644, 0.92117536, 0.94380003, 0.8927059 ,\n",
      "       0.9170504 , 0.95693374, 0.9624417 , 0.9211806 , 0.8114498 ,\n",
      "       0.9385171 , 0.90897804, 0.94225425, 0.93099254, 0.9139514 ,\n",
      "       0.85425746, 0.9152032 , 0.9445149 , 0.94936436, 0.9655917 ,\n",
      "       0.94787586, 0.91199   , 0.89200824, 0.9075272 , 0.9344207 ,\n",
      "       0.9007526 , 0.82058364, 0.92820233, 0.9136762 , 0.91172814,\n",
      "       0.92966086, 0.90445125, 0.90654355, 0.9200295 , 0.57346416,\n",
      "       0.91477096, 0.8996394 , 0.9488566 , 0.6038303 , 0.86766505,\n",
      "       0.9154449 , 0.9158241 , 0.9318366 , 0.9557074 , 0.9075396 ,\n",
      "       0.9452314 , 0.90977174, 0.96568984, 0.9337589 , 0.9207058 ,\n",
      "       0.93555814, 0.9225538 , 0.86442536, 0.9296161 , 0.89918876,\n",
      "       0.88338864, 0.9403448 , 0.94892496, 0.7476772 , 0.90686935,\n",
      "       0.89532304, 0.90588856, 0.91675234, 0.92574555, 0.933619  ,\n",
      "       0.6957282 , 0.9077637 , 0.93055224, 0.9230387 , 0.9050301 ,\n",
      "       0.89707875, 0.8886904 , 0.9109347 , 0.9118134 , 0.94409746,\n",
      "       0.9453328 , 0.9245956 , 0.949736  , 0.94053143, 0.9533398 ,\n",
      "       0.9481562 , 0.934288  , 0.70722765, 0.9215066 , 0.93794835,\n",
      "       0.9402891 , 0.9278729 , 0.94008   , 0.71718866, 0.9263207 ,\n",
      "       0.90549445, 0.8483239 , 0.9605736 , 0.9175371 , 0.8920369 ,\n",
      "       0.9266784 , 0.9480126 , 0.94405675, 0.92138344, 0.92463636,\n",
      "       0.93791765, 0.96294475, 0.9288376 , 0.93350214, 0.9673506 ,\n",
      "       0.9430882 , 0.922881  , 0.92531717, 0.93931574, 0.9618508 ,\n",
      "       0.9357145 , 0.93198144, 0.931671  , 0.9659759 , 0.92901546,\n",
      "       0.8378408 , 0.9138927 , 0.8842022 , 0.9085781 , 0.94390714,\n",
      "       0.9608307 , 0.93077034, 0.95095146, 0.9257862 , 0.9108988 ,\n",
      "       0.9265462 , 0.93443316, 0.92356634, 0.9421255 , 0.89759374,\n",
      "       0.91274625, 0.9363333 , 0.9788202 , 0.7270526 , 0.91462904,\n",
      "       0.9197094 , 0.91251504, 0.90889174, 0.9305268 , 0.7262767 ,\n",
      "       0.92375857, 0.9310413 , 0.90813124, 0.9315725 , 0.7867329 ,\n",
      "       0.9543096 , 0.9423283 , 0.9348078 , 0.94626015, 0.94768137,\n",
      "       0.9114451 , 0.9351324 , 0.91658497, 0.9543583 , 0.9437475 ,\n",
      "       0.9526555 , 0.89870226, 0.92276293, 0.9177625 , 0.95534855,\n",
      "       0.9269844 , 0.9504333 , 0.93601197, 0.91205204, 0.91104263,\n",
      "       0.9454217 , 0.92776173, 0.917161  , 0.9191079 , 0.893855  ,\n",
      "       0.93469644, 0.89230597, 0.93331724, 0.9374895 , 0.9425987 ,\n",
      "       0.92874694, 0.92155915, 0.7542645 , 0.9159699 , 0.919898  ,\n",
      "       0.90202314, 0.940833  , 0.9168962 , 0.7852186 , 0.9070603 ,\n",
      "       0.92148864, 0.9406915 , 0.91201925, 0.9024926 , 0.9147393 ,\n",
      "       0.9209595 , 0.9021102 , 0.92088264, 0.9242503 , 0.93560994,\n",
      "       0.9516699 , 0.71898043, 0.6526611 , 0.7538119 , 0.9464846 ,\n",
      "       0.9434551 , 0.8832768 , 0.9044551 , 0.9328118 , 0.95359284,\n",
      "       0.92385983, 0.9108734 , 0.81372184, 0.89295614, 0.6892466 ,\n",
      "       0.9506695 , 0.9392942 , 0.9415965 , 0.9581456 , 0.89135766,\n",
      "       0.9675591 , 0.9259035 , 0.9161735 , 0.9316897 , 0.91864884,\n",
      "       0.9275797 , 0.9288334 , 0.93183047, 0.91203153, 0.9276796 ,\n",
      "       0.90510243, 0.9184391 , 0.8943191 , 0.90217596, 0.94848025,\n",
      "       0.91483605, 0.9079802 , 0.9445223 , 0.8916456 , 0.9040439 ,\n",
      "       0.94036585, 0.91390306, 0.9343286 , 0.8984334 , 0.9261148 ,\n",
      "       0.76608294, 0.7109776 , 0.9190526 , 0.73293525, 0.9044099 ,\n",
      "       0.8886008 , 0.892796  , 0.93762445, 0.89143133, 0.9669437 ,\n",
      "       0.9377562 , 0.93777007, 0.89359146, 0.9474329 , 0.8969154 ,\n",
      "       0.9190184 , 0.9311759 , 0.9398162 , 0.9166939 , 0.63787866,\n",
      "       0.8598233 , 0.90735346, 0.9486966 , 0.90185034, 0.9247457 ,\n",
      "       0.9365455 , 0.9318688 , 0.929365  , 0.9265128 , 0.95564723,\n",
      "       0.916664  , 0.9180521 , 0.9406057 , 0.9325655 , 0.9160912 ,\n",
      "       0.9150927 , 0.95219994, 0.9091579 , 0.89942414, 0.9475752 ,\n",
      "       0.86333495, 0.9352756 , 0.8885645 , 0.9572776 , 0.9351649 ,\n",
      "       0.93285704, 0.57152987, 0.9560086 , 0.9259275 , 0.9361772 ,\n",
      "       0.9317161 , 0.9303707 , 0.9177034 , 0.93215907, 0.9372438 ,\n",
      "       0.91126466, 0.667952  , 0.9209246 , 0.9280347 , 0.9111457 ,\n",
      "       0.93482375, 0.9090212 , 0.9434133 , 0.9233384 , 0.6639257 ,\n",
      "       0.9390106 , 0.9446374 , 0.6572948 , 0.9522766 , 0.92192554,\n",
      "       0.94214046, 0.9075009 , 0.9412347 , 0.9026439 , 0.89171016,\n",
      "       0.9284032 , 0.92969203, 0.6105364 , 0.94273174, 0.92764837,\n",
      "       0.915298  , 0.9648155 , 0.76429576, 0.9461404 , 0.9295993 ,\n",
      "       0.9208059 , 0.95306754, 0.9603305 , 0.9297595 , 0.91970766,\n",
      "       0.4992068 , 0.89457947, 0.95130986, 0.9512641 , 0.89726305,\n",
      "       0.9360135 , 0.9004448 , 0.9428064 , 0.9386279 , 0.9388646 ,\n",
      "       0.93066406, 0.9188382 , 0.9425026 , 0.92265916, 0.95466167,\n",
      "       0.9306728 , 0.9413683 , 0.9426339 , 0.8990325 , 0.9368491 ,\n",
      "       0.9036824 , 0.9423922 , 0.93645906, 0.9197224 , 0.96480197,\n",
      "       0.9308109 , 0.96489704, 0.67498106, 0.5968139 , 0.5846453 ,\n",
      "       0.91021687, 0.9336862 , 0.9228647 , 0.87974584, 0.9054073 ,\n",
      "       0.8454054 , 0.9330728 , 0.93688625, 0.9114064 , 0.93164146,\n",
      "       0.9348068 , 0.9262688 , 0.65353787, 0.9189541 , 0.93513167,\n",
      "       0.98171544, 0.91129345, 0.9487152 , 0.9208274 , 0.95204943,\n",
      "       0.92180264, 0.909806  , 0.9024251 , 0.94873846, 0.92834604,\n",
      "       0.89218116, 0.88626254, 0.94819003, 0.85526407, 0.9172009 ,\n",
      "       0.92518276, 0.9174412 , 0.96000683, 0.9419978 , 0.93209004,\n",
      "       0.84265965, 0.94204116, 0.9136804 , 0.9206284 , 0.9073892 ,\n",
      "       0.91679084, 0.88476187, 0.9189959 , 0.93951   , 0.913119  ,\n",
      "       0.9660142 , 0.93087244, 0.8776695 , 0.9505036 , 0.8981507 ,\n",
      "       0.9225997 , 0.6895441 , 0.9500169 , 0.94278187, 0.9016626 ,\n",
      "       0.91839916, 0.91436535, 0.9436809 , 0.9287094 , 0.9535609 ,\n",
      "       0.93534464, 0.9456423 , 0.9206062 , 0.92462146, 0.9264606 ,\n",
      "       0.935288  , 0.5562345 , 0.9024719 , 0.91857225, 0.9244006 ,\n",
      "       0.9253528 , 0.9305847 , 0.92127514, 0.9209566 , 0.93225294,\n",
      "       0.7232428 , 0.9051168 , 0.9078131 ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(lm_block.get_layer(index=2).get_weights())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "bert_chinese_params = dict()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "for layer_num in range(12):\n",
    "    each_layer_params = dict()\n",
    "    attention_layer = model.get_layer(name=f'Transformer-{str(layer_num)}-MultiHeadSelfAttention')\n",
    "    attn_layer_norm = model.get_layer(name=f'Transformer-{str(layer_num)}-MultiHeadSelfAttention-Norm')\n",
    "    ffw = model.get_layer(name=f'Transformer-{str(layer_num)}-FeedForward')\n",
    "    ffw_layer_norm = model.get_layer(name=f'Transformer-{str(layer_num)}-FeedForward-Norm')\n",
    "\n",
    "    attention_layer_weights = attention_layer.get_weights()\n",
    "    attn_layer_norm_weights = attn_layer_norm.get_weights()\n",
    "    ffw_weights = ffw.get_weights()\n",
    "    ffw_layer_norm_weights = ffw_layer_norm.get_weights()\n",
    "    ffw_plus_ffw_layer_norm_weights = ffw_weights + ffw_layer_norm_weights\n",
    "\n",
    "    each_layer_params['attn'] = attention_layer_weights\n",
    "    each_layer_params['attn_norm'] = attn_layer_norm_weights\n",
    "    each_layer_params['ffw'] = ffw_plus_ffw_layer_norm_weights\n",
    "\n",
    "    bert_chinese_params[layer_num] = each_layer_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "token_embedding = model.get_layer(name='Embedding-Token')\n",
    "token_embedding_weights = token_embedding.get_weights()\n",
    "token_segment_embedding = model.get_layer(name='Embedding-Segment')\n",
    "token_segment_embedding_weights = token_segment_embedding.get_weights()\n",
    "position_embedding = model.get_layer(name='Embedding-Position')\n",
    "position_embedding_weights = position_embedding.get_weights()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "(21128, 768)"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embedding_weights[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "(2, 768)"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_segment_embedding_weights[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "(512, 768)"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embedding_weights[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "bert_chinese_params['word_emb'] = token_embedding_weights\n",
    "bert_chinese_params['token_type_emb'] = token_segment_embedding_weights\n",
    "bert_chinese_params['position_emb'] = position_embedding_weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "pickle_file = open('all_bert_chinese_L-12_H-768_A-12_params.pkl', 'wb')\n",
    "pickle.dump(bert_chinese_params, pickle_file)\n",
    "pickle_file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}